 Hi, we are here for lesson 24 and once again it's becoming a bit of a tradition now. We're joined by Johno and Tanishq, which is always a pleasure. Hi, Johno, hi Tanishq. Hello. So you guys are a great lesson. Yeah, you guys are looking for, finally, actually completing stable diffusion, at least the unconditional stable diffusion. Well, I should say no, even conditional. So conditional stable diffusion except for the clip from scratch. We should be able to finish today, time permitting. That is exciting. That is exciting. Yeah, all right. Let's do it. Jump in any time. You've got things to talk about. So we're going to start with a very hopefully named 26 diffusion unit. And what we're going to do in 26 diffusion unit is to do unconditional diffusion from scratch. And there's not really too many new pieces, if I remember correctly. So all the stuff at the start, we've already seen. So when I wrote this, it was before I had noticed that the carus approach was doing less well than the regular cosine schedule approach. So I was doing carus noise a fire. But this is all the same as from the carus notebook, which was 23. So we can now create a unit that is based on what diffusers has, which is in turn based on lots of other prior art. I mean, the code's not at all based on it, but the basic, this structure is going to be the same as as what you'll get into fuses. The convolution we're going to use is the same as the final kind of convolution we used for tiny image net, which is what's called the pre activation convolution. So the convolution itself happens at the end and the normalization activation happen first. So this is a pre act convolution. And I've got a unit resident block. So I kind of wrote this before I actually did the pre act version of tiny image net. So I suspect this is actually the same, quite possibly exactly the same as the tiny image net one. So maybe this is nothing specific about this for you net. This is just really a pre act convent a pre act resident block. So we've got the, the two cons as per usual and the identity cons. Now there is one difference, though, to what we've seen before for resident blocks, which is that this resident block has no option to do down sampling, no option to do a stride. This is always stride one, which is our default. So the reason for that is that when we get to the thing that strings a bunch of them together, which will be called down block, this is where you have the option to add down sampling. And if you do add down sampling, we're going to add a stride to convolution after the rest block. And that's because this is how yeah diffusers and stable diffusion does it. I haven't studied this closely. I don't know if you have no like where this idea came from or why I'd be curious, you know, the difference is that normally we would have the have we'd have average pooling here in this connection. But yeah, there's different approaches what. We're using a lot of the history of the diffusers unconditional unit is to be compatible with the DDPM weights that were released. And some follow and work on that. And I know like then improved it. And these others like they all kind of built on that same sort of unit structure, even though it's slightly unconventional if you're coming from like a normal computer vision background. And do you recall where the DDPM architecture came from because like some of the ideas came from some of the N units, but I don't know if DDPM. Yeah, they had something called efficient unit that was inspired by some prior work that I can't remember the like the lineage. And anyway, yeah, just the. The fuselblers want to since become you know, like you can add in parameters to control some of the stuff, but yeah, it's. We shouldn't assume that this is the optimal approach, I suppose. But I'll yeah, I will dig into the history and try and find out how much. What ablation studies have been done. So for those of you haven't heard of ablation studies, that's where you like try, you know, a bunch of different ways of doing things and score which one works better and which one works less well and kind of create a table of all of those options. And so where you can't find ablation studies for something you're interested in often that means that, you know, maybe not many other options were tried because researchers don't have time to try everything. Okay, now the unit, if we go back to the unit that we used for super resolution, we just go back to our basic version. What we did as we went down through the layers in the down sampling section. We stored the activations at each point into a list called layers. And then as we went through the up sampling, we added those down sampling layers back into the up sampling activations. So that's kind of basic structure of a unit. You don't have to add you can also concatenate and actually concatenating is what is. It's a bit of it's think it's more common nowadays, and I think original unit might have been concatenating. Although for super resolution, just adding seems pretty sensible. So we're going to concatenate that what we're going to do is we're going to try to, we're going to kind of exercise our Python muscles a little bit to try to see interesting ways to make some of this at all easier to. At turn different down sampling back bones into units. And you also use that as an opportunity to learn a bit more Python. So what we're going to do is we're going to create something called a saved res block and a saved convolution. And so our down our down blocks. So these are our res blocks containing a certain number of res block layers. By this optional stride to conf. We're going to use saved res blocks and saved columns. And what these are going to do is going to be the same as a normal convolution and the same as a normal res block, the same as normal unit res block. But they're going to remember the activations. And the reason for that is that later on in the unit, we're going to go through and grab those saved activations all at once into a big list. So. So then yeah, we basically don't have to kind of think about it. And so to do that, we create a class called a save module. And all save module does is it calls forward to grab the the res block or conf results and stores that before returning it. That's weird because hopefully you know by now that super calls the thing in the parent class that saved module doesn't have a parent class. So this is what's called a mix in. And it's using something called multiple inheritance. And mix ends are as it describes here, it's a design pattern, which is to say it's not particularly a part of Python per say it's a design pattern that uses multiple inheritance now what model inheritance is is where you can say, oh, this class could save res block inherits from two things save module and unit res block. And what that does is it means that all of the methods in both of these will end up in here. Now that would be simple enough except we've got a bit of a confusion here, which is that unit res block contains forward and save module contains forward. So it's all very well just combining the methods from both of them, but what if they have the same method. And the answer is that the one you list first can call when it calls forward it's actually calling forward in the later one. And that's why it's a mix and it's mixing this functionality into this functionality. So it's a unit res block where we've customized forward so it calls the existing forward and also saves it. So you see, mix in quite a lot in the Python standard library, for example, the, the basic HTTP staff, the basic, some of the basic thread staff, you know, with networking users. Multiple inheritance using this mix in pattern. So with this approach, then the actual implementation of saved res block is nothing at all. So pass means don't do anything. So this is just literally just a class, which has no nothing no implementation of its own other than just to be a mix in of these two classes. So a save convolution is an end end.com 2d with the save module mixed in. What's going to happen now is that we can call a saved res block just like a unit res block and a saved conf just like an end.com 2d. But that object is going to end up with the activations inside the dot saved attribute. So now a down sampling block is just a sequential of saved res blocks. As per usual, the very first one is going to have the number of in channels to start with. And we'll always have a number of an F, the number of filters output and then after that, the inputs will be illc equal to NF because the first ones change the number of channels. And we'll do that for however many layers we have. And then at the end of that process, as we discussed, we will add to that sequential a saved conf with strive to to do the down sampling here for requested. So we're going to end up with a single end of sequential for a down block. And then an up block is going to look very similar. But instead of using an end end.com 2d with a straight to up sampling will be done with a sequence of an upsampling layer. And so literally all that does is it just duplicates every pixel four times into little two by two grid. That's all an upsampling layer does nothing clever and then follow that by a stride one convolution. So that allows it to, you know, adjust some of those pixels is if necessary with a simple three by three conf. So that's pretty similar to a stride to down sampling. This is kind of the rough equivalent for up sampling. There are other ways of doing upsampling. This is just the one that stable diffusion does. So an up block looks a lot like a down block except that now. So as before, we're going to create a bunch of unit res blocks. These are not saved res blocks. Of course, we want to use the safe results in the upsampling path of the unit. So we just use normal res blocks. But what we're going to do now is as we go through each res net, we're going to call it not just on our activations, but we're going to concatenate that with whatever was stored during the down sampling path. So this is going to be a list of all of the things stored in the down sampling path. It'll be passed to the up block. And so pop will grab the last one off that list and concatenate it with the activations and pass that to the res net. So we need to know how many filters there were, how many activations there were in the down sampling path. So that's stored here is the previous number of filters in the down sampling path. And so the res block. Wanted to add those in. In addition to the normal number. So that's what's going to happen there. So yeah, do that for each layer as before. And then at the end. Add an up sampling layer. If it's been requested, so it's a bullion. Okay, so that's the up sampling block. Is that all makes sense so far? Yeah, looks good. Okay. So the unit now is going to look a lot like our previous unit. We're going to start out as we tend to with the convolution. To now allow us to create a few more channels. And so we're passing to our unit. That's just, you know, how many channels are in your image and how many channels are in your output image. So for normal image, normal full color images, that'll be three three. How many filters are there for each of those resident blocks, up blocks and down blocks you've got. And in the down sampling, how many layers are there in each block. So we go from the con will go from in channels, so it'll be three to at f zero, which this is the number of filters in the stable diffusion model. They're pretty big as you see by default. And so that's the number of channels we would create, which is like very redundant in that this is a three by three con so it only contains three by three by three channels equals 27 inputs and 224 outputs. So it's not you know, doing computation useful computation in a sense, it's just giving it more space to work with down down the line, which I don't think that makes sense, but I haven't played with it enough to be sure. Normally we would do like, you know, like a few res blocks or something at this level to more gradually increase it because this feels like a lot of wasted effort. Yeah, I haven't studied that closely enough to be sure. So Jeremy, just to tweet, this is the default, I think the default settings for the unconditional unit in diffusers, but the stable diffusion unit actually has even more channels that has 320 640 and then 1,280, 1,280. Cool. Thanks for clarifying and it's yeah, the unconditional one, which is what we're doing right now. That's a great point. Okay, so then we yeah, we go through all of our number of filters and actually the first res block contains 224 to 224. So that's why it's kind of keeping track of this stuff and then the second res block is 224 to 448 and then 448 to 672 and then 672 to 896. That's why we're just going to have to keep track of these things. So yeah, we add, so we have a sequential for our down blocks and we just add a down block. The very last one doesn't have down sampling, which makes sense, right, because the very last one, there's nothing after it's an point down sampling. Other than that, they all have down sampling. And then we have one more res block in the middle. And which is that the same as what we did. Okay, so we didn't have a middle res block in our original unit here. What about this one? Do we have any mid blocks? No, so we haven't done. Okay, but I mean, so it's just another res block that you do after the down sampling. And then we go through the reverse, Mr. filters and go through those and adding up blocks. And then one convolution at the end to turn it from 224 channels to three channels. Okay, and so the forward then is going to store in saved for the layers, just like we did back with this unit. But we don't really have to do it explicitly now. We just call the sequential model. And thanks to our automatic saving, each of those now will, we can just go through each of those and grab their dot saved. So that's handy. We then call that mid block, which is just another res block and then same thing. Okay, now for the apps and what we do is we just passed in those saved. Right, and just remember it's going to pop in the out. Each time. And then the conv at the end. So that's, yeah, that's it. That's our unconditional model. It's not quite the same as that if you use this unconditional model, because it doesn't have attention, which is something we're going to add. Next. But other than that, this is the same. So let's for because we're doing a simple problem, which is fashion, MNIST will use less channels than the default using two layers per block is standard. One thing to note, though, is that in the up sampling blocks, it actually is going to be three layers, none layers plus one. The reason for that is that the way stable diffusion and diffuses do it is that even the output of the down sampling is also saved. So if you have num layers equals to and there'll be two res blocks saving things here and one con of saving things here. So you'll have three saved cross connections. So that's why there's an extra plus one here. Okay, and then we can just train it using many AI has per usual. I didn't save it after I last trained it. Sorry about that. So just me at trained. Okay. Now that. Oh, okay. No, that is actually missing something else important as well as attention. The other thing it's missing is that thing that we discovered is pretty important, which is the time and getting. So we already know that sampling doesn't work particularly well with that time and betting. So I didn't even bother sampling this. I didn't want to add all the stuff necessary to make that work a bit better. I thought let's just go ahead and do time and betting. So time and betting. There's a few ways to do it. And the way it's done in stable diffusion is what's called sign your soil embeddings. The basic idea, maybe we'll skip ahead of it. The basic idea is that we're going to create a res block with embeddings where forward is not just going to get the activations, but it's also going to get T, which is a vector that represents the embeddings of each time step. So I actually beat it. It'll be a matrix because it's really in the batch. But for one element of the batch, it's a vector. And it's an embedding in exactly the same way as when we did an LP each token hadn't embedding. And so the word of the would have an embedding and the word John, I would have an embedding and the word Tunisian, could have an embedding. Although Tunisian could probably actually be multiple tokens until he's famous enough that he's mentioned in nearly every piece of literature, in which point Tunisian will get his own token. I expect. I'll tell you know when you've made it. So the time embedding will be the same T, T of time step zero will have a particular vector time step one will have a particular vector and so forth. Well, actually, we're doing carous so actually they're not time step one to three. There's there, they're actually sigma's, you know, so they're continuous. But same idea. So the specific value of sigma, which is actually what T is going to be slightly confusingly, we'll have a specific embedding. Now, we want two values of sigma or T, which are very close to each other should have similar embeddings. And if they're different to each other, they should have different embeddings. So how do we make that happen, you know, and also make sure there's a lot of variety of the embeddings across all the possibilities. So the way we do that is with these, these sign your sword all time steps. So let's have a look at how they work. So you first have to decide how, how big do you want your embeddings to be. Just like we do at NLP. But it is the word the is it represented by eight floats or 16 floats or 400 floats or whatever. That's just assume it's 16. So let's say we're just looking at the a bunch of time steps, which is between negative 10 and 10. And we're just to 100 of them. I mean, we don't actually have negative sigma's or T. So that's an exact exact sense, but doesn't matter. It's, you know, it's good to show you the idea. And so then we say like, OK, what's the largest time step you could have, well, I just sigma that you can have. Interestingly, every single model I found every single model I found uses 10,000 for this. Even though that number actually comes from the NLP Transformers literature. And it's based on the idea of like, OK, what's the maximum sequence length we support? You can have at 10,000 things in a, you know, in a document or whatever in a sequence. But we don't actually have a sigma that comes to 10,000. So I'm using the number that's used in real life in stable diffusion and all the other models. But it's interestingly, this is the here purely as I can tell as a historical accident, because this is like the maximum sequence length that NLP transformers people thought they would need to support. OK, now what we're then going to do is we're going to be then doing e to the power of a bunch of things. And so that's going to be our exponent. And so exponent is going to be equal to log of the period, which is about nine times the numbers between not and one. Eight of them, because we got three, so we want 16. So you'll see why we want eight of them and not 16 in a moment. But basically here are the eight exponents we're going to use. So then not surprisingly, we do e to the power of that. OK, so we do e to the power of that, each of these eight things. And we've also got the actual time steps. So imagine these are the actual time steps we have in our batch. So there's a batch of a hundred. And they contain these this range of sigma's or time steps. So to create our embeddings, what we do is we do a out of product of the exponent dot x and the time steps. This is step one. And so this is using a broadcasting trick we've seen before we had a unit access and access zero here. And at a unit access, at a, sorry, an access, sorry, an access one here. And at a unit access access access access zero here. So if we multiply those together, then it's going to broadcast this one across this access. And this one across this access. So we end up with a 100 by eight. So it's basically a Cartesian product or the possible combinations of time step and exponent multiplied together. And so he is like, you know, a few of those different exponents for a few different values. Okay, so that's not very interesting yet. We haven't yet reached something where each time step is similar to each next door time step. You know, over here, you know, these embeddings look very different to each other and over here. They're very similar. So what we then do is we take the sign and the cosine of those. So that is 100 by eight. And that is 100 by eight. And that gives us 100 by 16. So we can catenate those together. So that's a little bit hard to wrap your head around. So let's take a look. So the across the hundred time steps, 100 signals. This one here is the first sine wave. And then this one here is the second sine wave. And this one here is the third. And this one here is the fourth and the fifth. So you can see as you go up to higher higher numbers. You're basically, you know, stretching the sine wave out. And then once you get up to index eight. You're back up to the same frequency as this blue one, because now we're starting the cosine rather than sine and cosine is identical to sine. It's just shifted across a tiny bit. You can see these two light blue lines are the same. And these two orange lines are the same. They're just shifted across. I shouldn't say lines. So curves. So when we can catenate those altogether, we can actually draw a picture of it. And so this picture is 100 pixels across and 16 pixels top to bottom. And so if you picked out a particular point. So for example, in the middle here for T equals zero. Well, sigma equals zero. One column is an embedding. So the bright represents higher numbers and the dark represents lower numbers. And so you can see every column looks different, even though the columns next to each other looks similar. So that's called a time step embedding. And this is definitely something you want to experiment with. I really, I've tried to do the plots I thought are useful to understand this. And China and Tunisian also had ideas about plots for these, you know, which we've shown. But you know, the only way to really understand them is to experiment. So then we can put that all into a function where you just say, OK, well, how many times, sorry, what are the time steps? How many embedding dimensions do you want? What's the maximum period? And then all I did was I just copied and pasted the previous cells and merged them together. So you can see there's our out of product. And there's our cat of sign and cause. If you end up with a, if you have an odd number embedding dimension, you have to pat it to make it even don't worry about that. So here's something that now you can pass in the number of sorry, the actual time steps or sigmas and the number of embedding dimensions and you will get back. Something like this. It won't be a nice curve because your times your time steps in a batch won't all be next to each other. It's the same, the same idea. A little visualization there, which goes back to your comment about the max period being super high. Yeah. You said like adjacent ones are somewhat similar because that's what we want. But there is some change. But if you look all of this first 100. Some just like the half of the embeddings look like they don't really change at all. And that's because 50 to 100 on a scale of like 30,000, you want those to be quite similar because those are sold very early in this like super long sequence that these are designed for. Yeah. So here actually we've got wasted space. So here we've got a max period of a thousand instead of times the figure size, you can see it better and it's using up a bit more of the space. Or go to max period of 10. And it's actually now this is, yeah, using it much better. Yeah. So like based on what you're saying, Johno, I agree it like it seems like it would be a lot richer to use. These time step embeddings with a suitable max period or maybe you just wouldn't need as many embedding dimensions. I guess if you did use something very wasteful like this, but you used lots of embedding dimensions, then it's going to still capture some useful ones. Yeah. Thanks, Johno. So yeah. So this is one of these interesting little insights about things that are very deep in code, which are not sure anybody probably much looks at. Okay. So let's do a unit with time step embedding in it. So what do you do once you've got like this column, you know, embeddings for each the ultimate batch, what do you do with it? Well, there's a few things you can do with it. What stable diffusion does, I think this correct, I'm not promising to over remember all these details right, is that they make their embedding dimension length, as big as the number of activations. And what they, what we did, they didn't do is we can use chunk to take that and split it into two separate variables. So that just literally just the opposite of concatenate, it's just two separate variables. One of them is added to the activations and one of them is multiplied by the activations. So this is a scale and a shift. We don't just grab the embeddings as is the because each layer might want to do each, each, each, rest block might want to do different things with them. So we have a embedding projection, which is just a linear layer, which allows them to be projected. So it's projected from the number of embeddings to two times the number of filters so that that that towards dot chunk works. We also have an activation function called Cilio. This is the activation function that's used in stable diffusion. I don't think the details are particularly important, but it looks basically like a rectified linear with a slight curvey bit. Also known as switch. And it's just equal to X times sigmoid X. And yeah, I think it's like activation functions don't make a huge difference, but it'll. They can make things train a little better or a little faster and switch. It's been something that's worked pretty well. So people, a lot of people using switch or silly. I always call it swish. I think silly was actually where it was originally the, the, the gallery paper, which had silly was where it originally was kind of invented and maybe people didn't quite notice and then another paper called it swish and everybody called it swish and then people were like, wait, that wasn't the original paper. So I guess I should try to call it silly. Okay, so yeah, other than that, a res, it's just a normal res block. So we do our first con. Then we do our embedding projection of the activation function of time steps. And so that's going to be applied to every channel. Sorry to every pixel height and width. So that's why we have to add unit axes on the height and width that it's going to cause it to broadcast across those two axes. Do our chunk, do the scale and shift. Then we're ready for the second con and then we add it to the input with a additional con one stride one conviv necessary as we've done before if we have to change the number of channels. Okay. Yeah, because I like exercising a Python, not just muscles, I decided to use a second approach now. At the down block and the up block, I'm not saying which one's better or worse, we're not going to use. Modival inheritance anymore, but instead we're going to use. It's not even a decorator. It's a function which takes a function. What we're going to do now is we're going to use fun with 2d and embers block directly, but we're going to pass them to a function called saved. The function called saved is something which is going to take as input a callable, which could be a function or a module or whatever. So in this case, it's a module. Embers block or a con of 2d and it returns a callable. The callable it returns is identical to the callable that's passed into it except that it saves the result saved the activations saves result of the function. Where does it save it? It's going to save it into a list in the second argument you passed to it, which is the block. So the save function, you're going to pass it the module. We're going to grab the forward from it and store that away to remember what it was. And then the function that we want to replace it with, underscore f, I'm going to take some arguments and some keyword arguments. Well, basically it's just going to call the original modules. Forward passing in the arguments and keywords arguments. And we're then going to store the result in something called the saved attribute inside here. And then we have to return the result. So then we're going to replace the modules forward method with this function and return the module. And then the module is now being, yeah, I call it as a callable actually it can't be called what has to specifically be a module because with the forward that we're changing. This at wraps is just something which automatically it's from the Python standard libraries. It's just going to copy in the documentation and everything from the original forward so that it all looks like nothing's changed. Now, where does this dot saved come from? I realized now actually we could make this easier and automated, but I forgot didn't think of this at the time. So we have to create the saved here in the down block. It actually would have made more sense, I think here for it to have said if saved attribute doesn't exist and created. Which would look like this. If not has at your lock comma saved. Lock dot saved because if you do this, then you wouldn't need this anymore. Anyway, I didn't think of that at the time, so let's pretend that that's not what we do. Okay. So yeah, now the down sampling con and the res nets both contain saved versions of modules. We don't have to do anything to make that work. We just have to call them. We can't use sequential anymore because we have to pass in the time step to the res nets as well. It would be easy enough to create your own sequential for things with time steps, which passes them along. But that's not what we're doing here. Yeah, maybe it makes sense for sequential to always pass along all the extra arguments. But I don't think that's how they work. Yeah, so our up block is basically exactly the same as before, except we're now using ember as blocks instead. Just like before we're going to concatenate. So that's all the same. So, okay, so a unit model with time embeddings. Is going to look, if we look at the forward, the thing we're passing into it now is a tuple containing the activations and the time steps. A lot of sigma's in our case. So split them out. And what we're going to do is we're going to call that time step embedding function. We wrote saying, okay, these are the time steps and the number of embed the number of time step embeddings we want. Is equal to however many we asked for. And we're just going to set it equal to the first number of filters. That's all that happens there. And then we want to give the model the ability then to do whatever it wants with those to make those work the way it wants to. And the easiest smallest way to do that is to create a tiny little mlp. Which is going to take the time step and embeddings and return the actual embeddings to pass into the resident blocks. So tiny little mlp is just a linear layer with. And then we're going to add a linear layer. And then we're going to add a linear layer. That's interesting. I. I linear layer by default has an activation function. I'm pretty sure we should have act equals none. Here to be a linear layer and then an activation and then a linear layer. And then we're going to try re running. Okay. What would be the end of the world? It just means all the negatives will be lost here. Mix it half only half is useful. That's not great. Okay. And these are the kind of things like, you know, as you can see, you've got to be super careful of like, where do you have activation functions, where do you have batch norms? Is it pre activation post activation? It trains even if you make that mistake. And in this case, probably not too much performance, but often it's like, oh, you've done something where you accidentally zeroed up, you know, all except the last few channels of your. Like, I'll put some of something like that. And it will try anyway. It does the best uses what it can. Yeah. But yeah, it makes it very difficult. Make sure you're, you're not giving it those handicaps. Yeah, it's not like making a crud, or something. And you know that it's not working because the crashes or because like it doesn't show the username or whatever. Instead, you just get like slightly less good results. But since you haven't done it correctly in the first place, you don't know it's a less good results. Yeah, that's not really great ways to do this. It's really nice if you can have a existing model to compare to or something like that. Which is where Kaggle competitions work really well. Actually, if somebody's like got a Kaggle result, then you know that's a really good baseline. And you can check whether yours is as good as theirs. So, yeah, that's what this m n real P is for. So the down, yeah, the down and up box are the same as before the convert is the same as before. So, yeah, so we grab our time step embedding. So that's just that out of product pass through this signing soil, the sign and cosine. We then pass that through the mlp. And then we call our down sampling passing in those embeddings each time. Yeah, it's kind of interesting that we pass in the embeddings every time in the sense I don't exactly know why we don't just pass them in at the start. And in fact, in nlp, these kinds of embeddings, I think are generally just passed into the start. So this is kind of a curious difference. I don't know why it's you know, if there's been a place in studies or whatever. Do you guys know are there like any popular diffusion or generative models with time embeddings that don't pass them in or is this pretty universal? Some of the fancy architecture is like, I think we're currently interfacing networks and stuff just passing the conditioning. Oh, I'm actually not sure. Yeah, maybe they do still do it like at every state. I think some of them just take everything all at once up front and then do stack of transform block to something like that. I don't know if it's universal, but it definitely seems like all the unit style ones have this. The time set embedding going in that all that. We should try some in the ablations to see. Yeah, if it matters. I mean. So it doesn't matter too much either way. But yeah, if you didn't need it at every step, then it would maybe say if you were a bit of compute. Potentially. Yeah, so now the up sampling you're passing in the activations, the time step embeddings and those and that list of saved activations. So yeah, now we have a non attention stable diffusion unit. So we can train that. And we can sample from it. Using the same I just copied and pasted all the stuff from the carus notebook that we had. And there we have it. This is our first. diffusion. From scratch. And the. So we wrote every piece of code for this diffusion model. Yeah, I believe so. I mean, obviously in terms of the optimized code of implementation of. No, but yeah, we've written our version. We've written our version of everything here. I believe a big milestone. I think so. Yeah. And these fits are about the same as the fits that we get from the stable diffusion one. They're not particularly higher or lower. They bounce around a bit. So it's a little hard to compare. Yeah, they're basically the same. Yeah. So that's. That's it. That is an exciting step. And. Okay. Yeah. That's probably a good time to have a five minute break. Yeah. Yeah. Okay. Let's have a five minute break. Okay. Normally I would say we're back. But only some of us back. So. Janna is. Internet and electricity in some way is not the most reliable thing. And he seems to have disappeared. But we expect him to reappear at some point. We'll kick on. Janna Ellis. And hope that. The embubweights infrastructure. So it's itself out. All right. So. We're going to talk about attention. For a few reasons. Reason number one, very pragmatic. We said that we would replicate stable diffusion and the stable diffusion unit has attention in it. So we would be lying if we didn't do attention. Okay. Number two. Attention is one of the two basic building blocks of transformers. A transformer layer is attention attached to a one layer MLP. We already know how to create a one layer for one hidden layer MLP. So once we learn how to do attention, we'll know how to. We know how to create transform a box. So there's the two good reasons. I'm not including a reason which is our model is going to look a lot better with attention because I actually haven't had any success. So I'm not going to be able to see any success seeing any. Diffusion models I've trained work better with attention. So just to set your expectations. We are going to get it all working. But regardless of whether I use our implementation of attention or the diffuses one. It's not actually making it better. That might be because we need to use better types of attention than what diffuses has. Or it might be because it's just a very subtle difference that you only see on. Bigger images. I'm not sure that's something we're still trying to figure out. This is all pretty new. And not many people have done kind of the diffusions that the kind of oblation studies necessary to figure these things out. So. Yeah. So that's just life. Anyway, so there's lots of good reasons to know about attention. We'll certainly be using it a lot once we do an LP, which will be coming to pretty shortly. Pretty soon. And it looks like Jono is reappearing as well. So that's good. Okay. So let's talk about attention. The basic idea of attention. And what we're going to do is that we have. You know, an image. And we're going to be sliding a convolution kernel. Across that image. Right. And obviously we've got channels as well. Or filters. And so this also has that. And as we. Bring it across. We might be, you know, we're trying to figure out like what. What activations do we need to create to eventually, you know, correctly create outputs. But the correct answer as to what's here may depend on something that's way over here. And or something that's way over here. And so we're going to be able to see what it looks like. For example, if it's a cute little bunny rabbit. And this is where. It's ear is, you know. And there might be two different types of bunny rabbit that have different shaped ears. Well, we're really nice. To be able to see over here. What it's other ear looks like. For instance. With just convolutions that's challenging. It's not impossible. It's not the receptive field. And as you get deeper and deeper in a confnet, the receptive field gets bigger and bigger. But it's, you know, at higher up, it probably can't see the other ear at all. So it can't put it into those kind of more texture level layers. And later on, you know, even though this might be in the receptive field of here. Most of the weight, you know, the vast majority of the activations it's using is the stuff immediately around it. So what attention does is it lets you. Take a weighted average of other pixels. Around the image. Regardless of how far away they are. And so in this case, for example, why we might be interested in bringing in at least a few of the channels of these pixels over here. The way that attention is done in stable diffusion is pretty hacky and known to be suboptimal. But it's what we're going to implement because we're implementing stable diffusion and time for meeting. Maybe we'll look at some other options later. But the kind of attention we're going to be doing is 1D attention and it was attention that was developed for NLP. And NLP is sequences, one dimensional sequences of tokens. So to do attention, stable diffusion style, we're going to take this image and we're going to flatten out the pixels. So we've got all these pixels. We're going to take this row and put it here. And then we're going to take this row. We're going to put it here. So it's going to flatten the whole thing out. Into one big vector of all the pixels of row one and then all the pixels are very true. And then all the pixels are very true. Maybe it's column one column tree. I can't remember this row wise, but column wise, but it's flattened out. Anywho. And then it's actually for each image, it's actually, you know, a matrix, which I'm going to draw it a little bit 3D. Because we've got the channel dimension as well. So this is going to be the number across this way is going to be equal to the height times the width. And then the number this way is going to be the number of channels. Okay. So how do we decide, yeah, which, you know, bring in these other pixels. But what we do is we basically create a weighted average of all of these pixels. So maybe these ones get a bit of a negative weight. And these ones get a bit of a positive weight. And you know, these get a weight kind of somewhere in between. And so we're going to have a weighted average. And so basically each pixel. So let's say we're doing this pixel here right now is going to equal its original pixel plus. So it's called x plus the weighted average. So the sum across the matrices like x. I. Plus the sum of. Over all the other pixels. So from zero to. So we're going to have a weight time to width. Some weight. Times each pixel. The weights. They're going to sum to one. And so that way the, you know, the pixel value. And so that's not going to change. Well, that's not actually quite true. It's going to end up potentially twice as big a guess because it's being added to the original pixel. So attention itself is not with the x plus. But the way it's done in stable diffusion at least is it's is that the attention is added to the original pixel. So yeah, now I think about it. Anywho. So the big question is what values to use for the weights. And the way that we calculate those is we do a, we do a matrix product. And so our. For a particular pixel. We've got. You know, the number of channels for that one pixel. And what we do is we can compare that. To all of the number of channels for all the other pixels. So kind of this is pixel, let's say x one. And then we've got pixel number x two. Right. All those channels. We can take the dot product. Between those two things. And that will tell us how similar they are. And so one way of doing this would be to say like, okay, well, let's take that dot product for every pair of pixels. And that's very easy dot product to do. Because that's just what the matrix product is equal to. So if we've got. H. I w. I see. And then model player by its transpose. H. I w. Nice. Sorry. So it said transpose. And then totally failed to do transpose. Model play by its transpose. That will give us an H by w by H by w matrix. So each pixel or the pixels are down here. And for each pixel, as long as these. And then we can take the matrix to one. Then we've got a weight for each pixel. And it's easy to make these out of to one. We could just take this matrix multiplication. And take the sigmoid. Over the last dimension. And that makes it's not sigmoid. Man. It's wrong with me. Softmax, right? Yeah. And then we can take the matrix over the last dimension. And that will give me something that adds. The sum equals one. Okay. Now the thing is. It's not just that we want to find the places where. They look the same with the channels are basically the same. But we want to find the places where they're like. Similar in some particular way, you know. And so some particular set of channels similar in one to some different set of channels in another. And so, you know, in this case, we may be looking for the. Pointy iridness activations, you know, which actually represented by, you know, this, this and this, you know, and we want to just find those. So the way we do that is before we do this matrix product, we first. Put our matrix through. Through a projection. So we just basically put our matrix through a matrix multiplication. This one. So it's the same matrix, right? But we put it through two different projections. And so that lets it pick two different kind of sets of channels to focus on or not focus on before it decides, you know, oh, this pixel similar to this pixel in the way we care about. And then actually we don't even just multiply it then by the original pixels. We also put that through a different projection as well. So there's these different projections, well, the projection one projection to and projection three. And that gives it the ability to say like, oh, I want to compare these channels and, you know, these channels to these channels to find similarity and based on similarity, you then want to pick out these channels. Right, both positive and negative weight. So that's why there's these three different projections. And so the projections are called a, you. And V. Those are the projections. And so they're all being passed the same matrix. And because they're all being passed the same matrix, we call this self attention. OK, Johno, I know this is, I know you guys know this very well, but you also know it's really confusing. Did you have anything to add change anything else? Yeah, I like that you introduced this without resorting to the. Let's think of this as queries at all, which I think is, yeah, as we noted, using them. Actually, yeah, these are actually short for key query and value, even though I personally don't find those useful concepts. Yeah, you'll note on the scaling, you said, oh, so we said it so that the weights some to one. And so they need to worry about like are we doubling the scale of X. And because of that P3 aka V that projection that can learn to scale. This thing that's added to X appropriately. And so it's not like just doubling the size of X. It's increasing a little bit, which is why we scatter normalization in between all of these attention. That's good. And but it's not like as bad as it might be because we have that V projection. Yeah, that's a good point. And if this is a P3 or is actually the V projection is initialized such that it would have a mean of zero. And on average, you know, it should start out by not messing with our scale. Okay, so yeah, I guess I find it easier to think in terms of code. So let's look at the code. There's, you know, there's actually not much code. I think you've got a bit of background noise to Johno maybe. Yes, that's better. Thank you. So. So in terms of code, there's, you know, this is one of these things getting everything exactly right. And it's not just right. I wanted to get it identical to the stable diffusion. So we can say we've made it identical to stable diffusion. I've actually imported the attention block from diffuses so we can compare. And it is so nice when you've got an existing version of something to compare to to make sure you're getting the same results. So we're going to start off by saying let's say we've got a 16 by 16 pixel image. And this is some deeper level of activations. It's got 32 channels with a batch size of 64. So nc hw. I was going to use random numbers for now, but this has the, you know, reasonable dimensions for an activation inside a batch size 64 CNN or diffusion model or you know whatever. Okay, so the first thing we have to do. Is to flatten these out. And then we said in in 1d attention. This is just ignored. So it's easy to flatten things out. You just say.view. And you pass in the dimensions of the, the, this case, the three dimensions, we want, which is 6432 and everything else minus one means everything else. So x dot shape, colon two in this case is, you know, obviously it'd be easy just to type 6432. But I'm trying to create something that I can paste into a function later. So it's general. So that's the first two elements 6432. And then the start is inserts them directly in here. So 6432 minus one. So 16 by 16. Now then again, because this is all stolen from the NLP world in the NLP world things are. Have. Take all the sequence. So I'm going to call this sequence by which we're in height by width sequence comes before channel, which is often called D or dimension. So we then transpose those last two dimensions. So we've now got batch by sequence 16 by 16. By channel or dimension. So and they didn't only call this NSD sequence dimension. Okay, so we've got 32 channels. So we now need three different projections that go from 32 channels in to 32 channels out. So that's just a linear layer. Okay, and just remember a linear layer is just a matrix multiply plus a bias. So there's three of them. And so they're all going to be randomly initialized to different random numbers. We're called they're going to call them SKSQ SV. And so we can then they're just callable. So we can then pass the exact same thing into three or three because we're doing self attention to get back our keys queries and values. Or K Q and V. I just think of them as K Q and V because they're not really keys queries and values to me. So then we have to do the matrix multiply by the transpose. And so then for every one of the 64 items in the batch for every one of the 256 pixels there are now 256 weights. So at least there would be if we had done softmax, which we have it yet. So we can now put that into a self attention. As Johno mentioned, we want to make sure that we normalize things. So we can pop a normalization here. We talked about group norm back when we talked about batch norm. So group norm is just batch norm, which has been split into a bunch of sets of channels. Okay. So then we are going to create our K QV. Yeah, Johno. I was just going to ask should those be just bicycles false so that they're only a matrix multiplied to strictly match the traditional implementation. No, because they also do it that way. Yeah, they have bias in their attention box. Okay, so we've got our QK and V self. Q self. K self V being our projections. And so to do 2D self attention, we need to find the NCHW from your shape. We can do a normalization. We then do our flattening as discussed. We then transpose the last two dimensions. We then create our QKV by doing the projections. And we then do the matrix multiply. Now we've got to be a bit careful now, because as a result of that matrix multiply, we've changed the scale by by multiplying and adding all those things together. So if we then simply divide by the square root of the number of filters, it turns out that's, you can convince yourself with this if you wish to, but that's going to return it to the original scale. We can now do the softmax across the last dimension and then multiply each of them by V. So using matrix multiply to do them all in one go. We didn't mention, but we then do one final projection again, just to give it the opportunity to map things, you know, to some different scale, you know, shifted it also if necessary. We then transpose the last two back to where they started from and then we ship it back to where it started from and then add it. Remember, I said it's going to be x plus add it back to the original. So this is actually kind of self attention. Resonant style, if you like, diffuses, if I remember correctly, does include the x plus in there's but some implementations like, for example, pytorch implementation doesn't. Okay, so that's a self attention module and all you need to do is tell it how many channels to do attention on. So, and you need to tell it that because that's what we need for our four different projections and our group on and our scale. I guess strictly speaking, it doesn't have to be stored here, you could calculate it here. But anyway, it's not always fine. Okay, so if we create a self attention layer, we can then call it on our. It'll randomly generated numbers. And it doesn't change the shape because we transpose it back and reshape it back, but we can see that's basically worked. We can see it creates numbers. How do we know if they're right? Well, we could create a diffuses attention block. That will randomly generate a QKV projection. So actually they call something else they call a query key value projection attention and group norm. We call it QKV progenome. They're the same things. And so then we can just zip those tuples together. So that's going to take each pair of first pair, second pair, third pair. And copy the weight and the bias from their attention block, sorry, from our attention block to the diffuses attention block. And then we can check that they give the same value, which you can see they do. So this shows us that our attention block is the same as the diffuses attention block, which is nice. This is a trick which neither diffuses nor pie torch use for reasons I don't understand, which is that we don't actually need three separate attention three separate projections here. We could create one projection from N I to N I times three. That's basically doing three projections. So we could call this QKV. So that gives us 64 by 256 by 96 instead of 64 by 256 by 32. Because it's the three sets and then we can use chunk, which we saw earlier. The split that into three separate variables along the last dimension to get us how QKV. And we can then do the same thing Q at Q transpose, etc. So here's another version of attention where we just have one projection for QKV. And we chunkify it into separate QK and V. And this does the same thing, which is just a bit more concise. And should be faster as well, at least if you're not using some kind of XLA compiler or an X or Triton or whatever for normal pie torch. This should be faster because it's doing less back and forth between the CPU and the GPU. All right. So that's basic self attention. This is not what's done. Basically ever, however, because in fact, the kind of question of like, or which pixels do I care about. Different, different channels need to bring in information from different parts of the picture, depending on which channel we're talking about. And so the way we do that is with multi headed attention and multi headed attention actually turns out to be really simple and conceptually it's also really simple. What we do is we say, let's come back to when we look at C here and let's split them into four separate vectors, one, two, three, four. Let's split them. Right. And let's do the whole, you know, dot product thing on just, you know, here's the first part with the first part and then do the whole product part with the second part with the second part. And so forth. Right. So we're just going to do it separately, separate matrix model applies for different groups of channels. And the reason we do that is that then allows, yeah, different parts, different sets of channels to pull in different parts of the image. And so these different groups are called heads. And I don't know why they are. Does that seem reasonable? Anything to add to that? It's maybe worth thinking about why with just a single head specifically the softmax starts to come into play. Because you know, we said, oh, it's like a weighted sum to bring in information from different parts and whatever else. But with softmax, what tends to happen is whatever weight is highest gets scaled up quite dramatically. And so it's like almost like focused on just that one thing. And then yeah, like as you said, Jeremy, like different channels might want to refer to different things and, you know, just having this one like single weight. That's across all the channels means that that signal is going to be like focused on maybe only one or two things as opposed to being able to bring in lots of different kinds of information based on the different channels. Right. And you're pointing out, I was going to measure the same thing actually. That's a good point. So, so you're mentioning the second interesting important point about softmax, you know, point one is that it creates something that's to one. But point two is that because of its e to this, it tends to highlight one thing very strongly. And yes, so if we had single headed attention, your point guys, I guess is that you're saying it would end up basically picking nearly all one pixel, which would not be very interesting. Okay, awesome. Oh, I see where everything's got thick. I've accidentally turned it into a marker. Okay, so multi headed attention. I'll come back to the details, perhaps implemented in terms of, but I was going to mention the basic idea. This is multi headed attention and this is identical to before except I've just stored one more thing, which is how many heads do you want. And then the forward is actually nearly all the same. So this is identical, identical, identical, this is new, identical, identical, identical, new, identical, identical, identical. So there's just two new lines of code, which might be surprising, but that's all we needed to make this work. And they're also pretty wacky, interesting new lines of code to look at. Conceptually, what these two lines of code do is they first they do the projection. And then, they basically take the number of heads. So we're going to do four heads. We've got 32 channels, four heads. So each head's going to contain eight channels. And they basically grab, they're going to we got to keep it as being eight channels, not a 32 channels. And we're going to make each batch four times bigger, right, because the images in a batch don't combine with each other at all. They're totally separate. So instead of having one image containing 32 channels, we're going to turn that into four images containing eight channels. And that's actually all we need, right, because remember I told you that each group of channels, each head, we want to have nothing to do with each other. So if we literally turn them into different images, then they can't have anything to do with each other, because batches don't interact to each other at all. So, these rearrange this rearrange and I explain how this works in a moment, but it's basically saying, think of the channel dimension as being of h groups of D and rearrange it. So instead the batch channel is n groups of h. And the channels is now just D. So that would be eight instead of four by eight. And then we do everything else exactly the same way as usual. But now that group that the channels are split into groups of h groups of four. And then after that, okay, well, we were thinking of the batches as being of size n by h. Let's now think of the channels as being of size h by D. That's what these rearrange as to. So let me explain how these work in the diffuses code. I've, I can't remember if I duplicated it or just inspired by it, they've got things called heads to batch and batch to heads, which do exactly these things. And so if a heads to batch, they say, okay, you've got 64 per batch by 256 pixels by 32 channels. Okay, let's reshape it. So you've got 64 images by 256 pixels by four heads by the rest. So that would be 32 over eight channels. So it's split it out into a separate dimension. And then if we transpose these two dimensions, it'll then be n by four. So n by heads by s, so by minus one. And so then we can reshape. So those first two dimensions get combined into one. So that's what heads to batch does. And batch to heads does the exact opposite, right, reshape to bring the batch back to here and then heads by SL by D. And then transpose it back again and reshape it back again so that the heads gets it. So this is kind of how to do it using just traditional pie torch methods that we've seen before. But I wanted to show you guys this new ish library called inops inspired as it suggests by Einstein summation notation, but it's absolutely not Einstein summation notation. It's something different. And the main thing it has is this thing called rearrange. And rearrange is kind of like a nifty rethinking of Einstein summation notation as a tensor rearrangement notation. So we've got a tensor called t we created earlier 64 by 256 by 32. And what inops rearrange does is you pass it this specification string that says turn this into this. Okay, this says that I have a rank three tensor three dimensions three axes containing the first dimension is of length n. The second dimension is of length s. The third dimension is in parentheses is of length h times D. Where h is eight. Okay, and then I want you to just move things around. So so that nothing is like broken, you know, so everything shifted correctly into the right spots. So that we now have each each batch is now instead n times eight n times each. The sequence length is the same. And D is now the number of channels previously the number of channels was h by D. Now it's D's the number of channels has been reduced by a factor of eight. And you can see it here it's turned T from something of 64 by 256 by 32 into something of size 64 times eight. By 256 by 32 divided by eight. And so this is like really nice because you know a this one line of code to me is clearer and easier and I like writing it better than these lines of code. But where it's particularly nice is when I had to go the opposite direction. I literally took this cut it. Put it here and put the arrow in the middle like it's literally backwards, which is really nice right because we just rearranging it in the other order. And so if we rearrange it in the other order, we take our 512 by 256 by 4 thing that we just created and end up with a 64 by 256 by 32 thing, which we started with. And confirm that the end thing equals every element equals the first thing. So that shows me that my rearrangement has returned its original correctly. Yeah, so I'm already had a detention of already shown you it's the same thing as before, but. Pulling everything out into the batch for each head and then pulling the heads back into the channels. So we can do multi-headed attention with 32 channels and four heads. And check that all looks okay. So pie torch has that all built in. It's got an end of multi-head attention. Be very careful. Be more careful than me. In fact, because I keep forgetting that it actually expects the batch to be the second dimension. So make sure you write batch first equals true to make batch the first dimension and that way it'll be the same as diffuses. I mean, it might not be identical, but the same. It should be almost the same. The same idea. And to make it self-attention, you've got to pass in three things, right? So the three things will all be the same for self-attention. This is the thing that's going to be passed through the. You have to keep the key projection, the key projection and the V projection. And you can pass different things to those. If you pass different things to those, you'll get something called cross attention rather than self attention, which. I'm not sure we're going to talk about until we do it in an LP. Just on the rearrange thing. I know that if you've been doing your pytorch and you're used to like you really know what transpose and. You know, reshape and whatever do. Then it can be a little bit weird to see the same notation. But once you get into it, it's really, really nice. And if you look at this off attention, multi-headed implementation there, you've got dot view and dot transpose and that reshape. It's quite fun practice. Like if you're just saying, oh, this I'm not saying looks really useful. Like taking an existing implementation like this and say, oh, maybe instead of like, can I do it instead of dot reshape or whatever, can I start replacing these individual operations with the equivalent like rearrange call. And then checking it the opposite of the same. That's what what helped it like click for me was, oh, okay. Like I can start to express if it's just transpose, then that's a rearrange with the last two channels. Yeah, I only just started using this and I've obviously had many years of using reshape transpose, et cetera in piano, TensorFlow, carousel, pie torch, apl. And I would say within 10 minutes, I was like, oh, I like this much better. You know, like it's fine for me at least it didn't take too long to be convinced. It's not part of play torture and I think you've got a pip install it by the way. And it seems to be coming super popular now, at least in the kind of diffusion research crowd everybody seems to be using. I know suddenly, even that's been around for a few years. And actually put in an issue there and asked them to add in Einstein summation notation as well, which they've now done. So it's kind of like your one place for everything, which is great. And it also works across. TensorFlow and other libraries as well. Just nice. Okay, so we can now add that to our unit. So this is basically a copy of the previous notebook, except what I've now done is I did this at the point where it's like, oh, yeah, turns out the cosine. And this is the thing's better. So I back to cosine schedule. Now this is copied from the cosine schedule book. And we're still doing the minus point five thing because we love it. And so this time I actually decided to export stuff into a mini AI dot diffusion. So since so this point of salt like things are working pretty well. And I so I renamed unit comes to precon. It's been a name. And betting has been exported. Up samples been exported. This is like a pre act linear version exported. I tried using an end of multi head attention and it didn't work very well for some reason. So I haven't figured out why that is yet. So I'm using. Yeah, this self attention, which we just talked about. And we have to divide the number of channels by the number of heads because the effective number of heads is, you know, divided across and heads. Yeah, that's the thing. Yeah, that's what the fuses does, I think it's not what an end multi head attention does. And actually I think, and I divided by and I divided by attention chance is actually just equal to attention chance. Anyway, that's mine. Yeah, so, okay, so that's all copied in from the previous one. The only thing that's different here is I haven't got the dot view minus one thing here. So this is a one D self attention and then 2D self attention just adds the dot view before we call forward and then dot reshape it back again. So yeah, so we've got one day and 2D self attention. Okay, so now our M res block has one extra thing you can pass in which is attention channels. And so if you pass in attention channels, we're going to create something called self dot attention, which is a self attention to the layer with the right number of filters and the requested number of channels. And so this is all identical to what we've seen before, except if we've got attention, then we add it. Oh, yeah, and the attention that I did here is the non res netty version. So we have to do x plus because that's more flexible, you can then choose to have it or not have it this way. Okay, so that's an ember as block with attention. And so now our down block, you have to tell it how many attention channels we want because the rest of the res blocks need that the up block, you have to know how many attention channels we want because again the rest blocks need that. And so now the unit model, where does the attention go? Okay, we have to say how many attention channels you want. And then you say which index block do you start adding attention? So why don't we, so then what happens is the attention is done. Here, we each resident has attention. And so as we discussed, you just do the normal res and then the attention. Right. And if you. What that in at the very start, right, so you've got a 256 by 256 image. 256 by 256 image. Then you're going to end up with this matrix here is going to be 256 by 256 on one side and 256 by 256 on the other side and contain however many, you know, NF channels. That's huge. And you have to back prop through it. So you have to store all that to allow that prop to happen. It's going to explode your memory. So what happens is basically nobody puts attention in the first layers. So that's why I've added a attention start, which is like at which block do we start adding attention? It's not zero for the reason we just discussed. Another way you could do this is to say like at what grid size should you start adding attention? And so generally speaking, people say when you get to 16 by 16, that's a good time to start adding attention. Although stable diffusion at 32 by 32. Because remember, they're using latens, which we'll see very shortly. I guess in the next lesson. So it starts at 64 by 64 and then they add attention at 32 by 32. So we're again, we're replicating stable diffusion here. So what a fusion uses attention start at index one. So we, you know, when we go self dot down, start a pan with the down block has zero attention channels. If we're not up to that block yet. And did oh on the up block. So we have to count from the end blocks. Now I think about it that should have attention as well. Let me look. So that's missing. Yeah, so the forward actually doesn't change at all for attention. It's only the in it. Yeah, so we can train that. So previously, yeah, we got without attention. We got to 137. And with attention. Oh, we can't compare directly because we've changed from carus to cosine. You can compare the sampling though. So we're getting what are we getting for five. It's very hard to tell if it's any better or not because. Well, again, you know, cosine schedules better. But yeah, when I've done kind of direct like with like I haven't managed to find any obvious improvements. So I'm not going to add any attention, but I mean, it's doing fine. You know, four is great. Yeah. All right. So then finally, did you want to add anything before we go into a conditional model. I was just going to make a note that like I guess just to clarify it, the, with it for the attention part of the motivation was certainly to do the sort of spatial mixing and kind of like, yeah, to get from different parts of the image and mix it. The problem is if it's too early where you do have one of, you know, the more individual pixels. Then the memory is very high. So it seems like you have to get that balance of where you don't kind of wanted to be early. So you can do some of that mixing, but you don't want to be too early where then the memory usage is too high. So it seems like there is certainly kind of the balance of trying to find maybe that right place where to add attention into your network. I was just thinking about that and maybe that's a point worth noting. Yeah, for sure. There is a trick, which is like what they do in, for example, vision transformers or the DIT, the diffusion, diffusion with transformers, which is that if you take like a six, eight by eight patch of the image. And then you flatten that all out or you run that through some like convolutional thing to turn it into a one by one by some larger number of channels. But you can reduce the spatial dimension by increasing the number of channels. And that gets you down to like a manageable size where you can then start doing attention as well. So that's another trick is like patching where you take a patch of the image. Right, right. And that's a number of like some embedding dimension or whatever you like to think of it, that as a one by one rather than an eight by eight or 16 by 16. And so that's how like you'll see, you know, 32 by 32 patch models, like some of the smaller clip models or 14 by 14 patch, some of the larger like the IT classification models and things like that. So that's another. I guess that's the yeah, that's mainly used when you have like a full transformer network, I guess, and then this is one where we have that sort of incorporating the attention into a convolutional network. So there's suddenly, I guess yeah, for different sorts of networks, different tricks. But yeah. Yeah. And I haven't decided yet if we're going to look at the IT or not, maybe we should based on what you're describing. I was just going to mention though that since you mentioned transformers. We've actually now got everything we need to create a transformer is here's a transformer block within blade and within beddings. And a transformer block within beddings is exactly the same embeddings that we've seen before. And then we add attention as we've seen before, there's a scarline shift. And then we pass it through an MLP. Which is just a linear layer, an activation, a normalize and a linear layer. For whatever reason, this is, you know, gal you, which is just another activation function is what people always using transformers. For reasons that suspect don't quite make sense and vision, everybody uses layer norm and again, I was just trying to replicate an existing paper, but this is just a standard MLP. So if you do. So in fact, if we get rid of the embeddings. Just to show you a true pure transformer. Okay, here's a pure transformer block. Right. So it's just normalize attention, add normalize model, perception, add that's all a transformer block is. And what's a transformer network, a transformer network is a sequential of transformers. And so in this diffusion model, I replaced my mid block with a list of, you know, it's going to sequential transformer blocks. So that is a transformer network. And to prove it, I then replay, this is another version, which I replaced that entire thing with the pie torch. Transform transformers encoder, just this is called encoder. This is just taken from pie torch. And so that's, that's the encoder and I just replaced it with that. So yeah, we've now built. Transformers now. Okay, why aren't we using them right now? And why did I just say I'm not even sure if we're going to do VIT, which is vision transformers. The reason is that transformers. You know, they're doing something very interesting, right, which is. Remember, we're just doing one D versions here, right. So. Transformers are taking something where we've got a sequence, right, which in the, in our case is pixels, high by width, but risk of the sequence. And every tree, everything in that sequence has a bunch of channels, or dimensions. I'll draw them all, but you get the idea. And so for each element of that sequence, which in our case, it's, you know, it's just some particular pixel. Right, and these are just the filters, channels, activations, whatever activations, I guess. What we're doing is the, we first do attention, which, you know, remember, there's a projection for each. So like it's mixing the channels a little bit, but just putting that aside. The main thing it's doing is each row is getting mixed together, you know, into a weighted average. And then after we do that, we put the whole thing through a multi layer perceptron. What the multi layer perceptron does is it entirely looks at each pixel on its own. So let's say this one. And puts that through linear activation norm linear, which we call an MLP. So a transformer network is a bunch of transformer layers. So it's basically going attention, MLP attention, MLP tension, etc, etc, MLP. That's all it's doing. And so in other words, it's mixing together the pixels or sequences, and then it's mixing together the channels, and it's mixing together the sequences and the mixing together the channels. And it's repeating this over and over. Because of the projections being done in the attention, it's not just mixing the pixels, but it's kind of it's, it's largely mixing pixels. And so this combination is very, very, very flexible. And it's flexible enough that it provably can actually approximate any convolution that you can think of. Given enough layers and enough time and learning the right parameters. The problem is that for this to approximate a combination requires a lot of data and a lot of layers and a lot of parameters and a lot of compute. So if you try to use this, so this is a transformer network transformer architecture, if you pass images into this, the past image in and try to predict, say, from image net, the class of the image. So use SGD to try and find weights for these attention projections and MLPs. If you do that on image net, you will end up with something that does indeed predict the class of each image, but it does it poorly. Now, it doesn't do it poorly because it's not capable of approximating a convolution. It does it poorly because image net, the entire image net as an image net one K is not big enough. To for a transformer learn how to do this. However, if you pass it a much bigger data set, many times larger than image net one K, then it will learn to approximate this very well. And in fact, it'll figure out a way of doing something like convolutions that are actually better than convolutions. So if you then take that, so that's going to be called a vision transformer of the OT. That's been free trained on a data set much bigger than image net, and then you fine tune it on image net. You will end up with something that is actually better than resident. And the difference is better than resident is because these combinations, right, which together when combined can approximate a convolution, these transformers, you know, distributions are our best guess as to like a good way to kind of represent the calculations we should do on images. But there's actually much more sophisticated things you could do, you know, if you're a computer and you could figure these things out better than a human can. And so a VIT actually figures out things that are even better than convolutions. And so when you fine tune image net using a very, you know, a VIT that's been pre trained on lots of data, then that's why it ends up being better than a resident. So that's why, you know, the things I'm showing you are not the things that contain transformers and diffusion because to make that work would require pre training on a really, really large data set for a really, really long amount of time. So anyway, so we're, we might only come to transformers, not in a very long time, but when we do do them in NLP in vision, maybe we'll cover them briefly, you know, they're very interesting to use as pre trained models. The main thing to know about them is, yeah, a VIT, you know, which is a really successful and when pre trained on lots of data, which are your nowadays is a very successful architecture, but like literally the VIT paper says, oh, we wondered what would happen if we take a totally plain one D transformer. You know, and convert it and use convert make it work on images with as few changes as possible. So everything we've learnt about attention today and MLPs applies directly because they haven't changed anything. And so one of the things you might realize that means is that you can't use a VIT that was trained on 224 by 224 pixel images on 128 by 128 pixel images because, you know, all of these self attention things are the wrong size, you know, and specifically the problem is actually the actually it's not really the attention, let me take that back. The all of the, the position embeddings are the wrong sides. And so actually that's something I, sorry, I forgot to mention is that in transformers, the first thing you do is you always take your, you know, these pixels and you add to them a position or embedding. And that's done, I mean, they can be done lots of different ways, but the most popular way is identical to what we did for the time step embedding it the sign you saw it all embedding. And so that's specific, you know, to how many, how many pixels there are in your image. So, yeah, that's an example of one of the things that makes the IT's little tricky. Anyway, but hopefully, yeah, you get the idea that we've got all the pieces that we need. Okay, so with that discussion, I think that's officially taken us over time. So maybe we should do the conditional next time. Do you know what actually it's tiny, let's just quickly do it now, if you guys got time. Yeah. Okay, so let's just, yeah, let's finish by doing a conditional model. So for a conditional model, we're going to basically say, I want something where I can say, draw me a number, sorry, draw me a shirt or draw me some pants or draw me some sandals. So we're going to pick one of the 10 fashion eminus classes and create an you know, create an image of a particular class. To do that, we need to know what class each thing is. Now, we already know what class each thing is because it's the the Y label, which way back in the beginning of time, we set, okay, it's just called the label. So that tells you what category it is. So we're going to change our collection function. So we call noise a fire as per usual, that gives us our noise image, our time step and our noise. But we're also going to then add to that tuple. What kind of fashion item is this? And so the first tuple will be noise, image noise and label and then the dependent variable as per usual is the noise. And so what's going to happen now when we call our unit, which is now a conditioned unit model. Is the input is now going to contain not just the activations and the time step, but it's also kind of contain the label. Okay, that label will be a number between zero and nine. So how do we convert the number between zero and nine into a vector, which represents that number. Well, we know exactly how to do that. And end on embedding. Okay, so we did that lots in part one. So let's make it exactly, you know, the same size as our time embedding. So in number of, number of activations in the embedding. It's going to be the same as our time step embedding. And so that's convenient. So now in the forward, we do our time step embedding is usual. We'll pass the labels into our conditioned embedding. The time embedding we'll put through the embedding link and before and then we're just going to add them together. And that's it, right? So this now represents a combination of the time and the fashion item plus. And then everything else is identical in in both parts. So all we've added is this one thing. And then we just literally sum it up. So we've now got a joint embedding representing two things. And then yeah, and then we train it. And you know, interestingly, it looks like the loss ends up the same, but it's, you know, the, you don't often see point 031. You know, it's, it's, it is a bit easier for it to do a conditional embedding model, because you're telling it what it is. Just that makes it a bit easier. So then to do conditional sampling. You have to pass in what type of thing do you want? So we're going to do these labels. And so then we create a vector just containing that number repeated over many times there on the batch. And we pass it to our model. So our model is now learned how to denoise something of type C. And then we're going to be like, oh, trust me, this noise contains is noise image of type C. It should hopefully denoise it into something of type C. That's, that's all there is to it. There's no magic there. So yeah, that's all we have to do to change the sampling. So like we didn't have to change DDIM step at all. So we, all we did was we added this one line of code and we added it there. So now we can say, okay, let's say class ID zero, which is t shirt top. So we're past that to sample. And there we go. Everything looks like t shirts and tops. Yeah, okay, I'm glad we didn't leave that to our next time because it's we can now say we have successfully replicated everything in stable diffusion except for being able to create whole sentences, which is what we're doing with claiming. Getting really close. Yes. Well, except that paper. All of it out here. So I guess we'll. We might do that next door, depending on how research goes. All right, we still need to do latent diffusion part. Oh, good point. Latents. Okay, we'll definitely do that next time. So let's see. So we're going to do a VAE. And latent diffusion. Which isn't enough for one lesson. So maybe some of the research on doing well end up in the next lesson as well. Yes. Okay, thanks for the reminder. Well, that we've already kind of done auto encoders. So VAE is going to be pretty. Pretty easy. Well, thank you to niche can jono fantastic comments. As always, glad your internet slash power reappeared, Johno. Back up. Yes. All right. Thanks gang. Cool. Thanks. Thanks.
