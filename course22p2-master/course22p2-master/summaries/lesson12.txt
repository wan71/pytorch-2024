Hi everybody, welcome back to Lesson 12 of Practical Deep Learning for Coders. So got a lot of stuff to cover today, so let's dive straight in. And I actually thought I would start by sharing something which  I've seen been getting a lot of attention recently, which is the CLIP Interrogator. So the CLIP Interrogator is a Hugging Face Spaces, I guess, Gradio app where I uploaded my image here  and its output, let's just zoom in a bit,  its output a text prompt for creating a CLIP embedding from, I guess. So I've seen a lot of folks on Twitter and elsewhere on the internet  saying that this is producing the CLIP prompt that would generate this image. And generally speaking, the CLIP, the prompts it creates are rather rude. My one's less rude than some, although, you know, extremely long forehead, maybe not,  thanks very much, but your personal data avatar, funny professional photo. I don't know what tectonics has been to me here without eyebrows. So this doesn't actually return the CLIP prompt that would generate this photo at all. And the fact that some people are saying that makes me realize that some people have no idea  what's going on with Stable Diffusion. So I thought we might take this as an  opportunity to explain why we can't do that and what we can try and do instead. So let's imagine that my friend took a photo and —of himself— and he wanted to send me his photo  and he thought he would compress it a whole lot. So what he did was he put it through the  CLIP image encoder. Okay. So that's going to take this big image  and it's going to turn it into an embedding. And the embedding is much,  much smaller than the image. It's just a vector of a few floats. So then my friend hopes that they could send me this embedding. And so they send that over in an email and they say, there you go, Jeremy, there's the CLIP  embedding of the photo I wanted to send you. So now you just have to  decode it to turn it back into a picture. So now I've got the embedding and I have to decode it. How would you do that?  Well, you can't. We have a function here, let's call it f, which  is the clip image encoder, which takes as input an image, which I'll call x and returns an embedding. Does that mean that there is some other function and inverse functions —we normally write with a  minus one—, an inverse function with which I can take that embedding, let's say we call that y,  we pass it y and it would give us back our photo. And so y, remember, is f(x). So to put it another way,  this is f inverse of f of f of y. So an inverse function is something that  undoes a function and so that gives you back y. Is there an inverse function  for the CLIP image encoder? Well, not everything has an inverse function. For example, consider the function like, let's say in Python,  which takes def f of x, returns zero. Can you invert that function,  if you get back, you pass in three, you get back zero, is there a function, oopsie, zero,  is there a function that's gonna take the output and give you back the input? No, of course not,  because you just threw the whole thing away. So not all functions can be inverted. And indeed, in this case, we've started with a function, which is whatever 512 by 512 by 3, say,  and we've turned it into something much, much smaller. I can't remember exactly how big a CLIP image encoding is,  embedding is, but it's much smaller. So clearly we're losing something. But what I could do is I could put it through a diffusion process. And so remember, a diffusion process is something where we have learned, we have  taught or… we shouldn't, I don't know if taught, well an algorithm has learned to take some noise. So we could start with some noise, and we could start with an image embedding. We haven't done this before, but we could do that. We could train something that takes noise and  image embedding and removes a bit of the noise. And we could run that a bunch of times. And it wouldn't give us back the original picture, but hopefully it would give us something back  if it's a conditional. So remember, using the conditional  diffusion approach, we'd get back something that might be something like our original image. So that's what diffusion is, right? Diffusion is something that takes an embedding  and inverts an encoder to give you back something that hopefully might generate that embedding. Now, of course, remember, we don't actually get image embeddings when we do  prompts in stable diffusion. Instead, we have text embeddings. But if you remember, that actually doesn't matter. Because do you remember how we actually,  well “we”, OpenAI, trained CLIP so that they had various pictures along with their captions,  and they trained an algorithm that was explicitly designed to make it so that each image returned a  embedding for the image that was similar to the embedding  that the text encoder created for the caption. And remember, all of the stuff that didn't match, it was trained to be different. And so that means that a text embedding, which describes this picture,  and the actual image embedding of this picture should be very similar if they're CLIP embeddings. That's the definition of CLIP embeddings. So you see this idea that you could take  a text or image embedding and turn it back into an image perfectly makes no sense. This is the very definition of the thing we're trying to do when we do CLIP. And because what we're basically trying to do is invert the embedding function,  these kinds of problems are generally referred to as “inverse problems”. So Stable Diffusion is something that attempts to approximate the solution to an inverse problem. So why does that mean that CLIP interrogator is not actually inverting the picture to give us back  the text? Well, it's just as nonsensical. If we've got an image embedding,  right, trying to undo that to get back to the picture and trying to undo that to get back  to a suitable prompt is equally infeasible. Both of them require inverting an encoder. And that just doesn't exist. The best we can do is, or at  least the best we know how to do at the moment, is to approximate that using a diffusion process. Okay, so that's why these texts that it spits back are fun and interesting, but they are not  the thing that you can put back into Stable Diffusion and have it generate the same photo. And the nice thing is that actually the code for this is available. And you can take a look at it. Here's the app. And you'll see what it does is it has a big list of —let's have a look at some examples. So it has a big, this has big lists of examples, for example, a big list of artists. And it has a big list of mediums and a big list of movements, and so forth. It's got all this hard coded pieces of text. And so what it does is it basically mixes  and matches those various things together to see which one works well. And it combines it with the output of something called the BLIP language model,  which is not designed to give you an exactly accurate description of an image,  but it has been specifically trained to give an okay-ish caption for an image and it actually  works reasonably well. But again, it's not  it's not the inverse of the clip encoder. So okay, so that's how that all works. So where we had got to was that we had done matrix multiplication with broadcasting,  where we had broadcast the entire column from the right hand matrix all at once. And that allowed us to get it down to a point where we only have one for loop written in Python. And generally speaking, we do not want to be doing loop— looping through too many things  in Python, because that's the slow bit. So the two inner loops we originally had,  which just to remind us,  originally were here, these two inner loops, looping through 10, and then to 784 respectively,  have been replaced with a single line of code. So that was pretty great. And our times now is  increased— is improved by 5,000 times. So we're 5,000 times faster than we started out. So another trick that we can use, which I'm a big fan of, is something called Einstein summation. And Einstein summation is a compact representation for representing products and sums. And this is an example of an Einstein summation. And what we're going to do now is we're going to  replicate our matrix product with an Einstein summation. And believe it or not, the entire thing can be pushed down to just  these characters, which is pretty amazing. So let me explain what's happening here. The arrow is separating the left hand side from the right hand side. The left hand side is the inputs. The right hand side is the output. The comma is between each input. So there are two inputs. The letters are just names that you're giving to the number of rows and the number of columns. So the first matrix we're multiplying by has i rows and k columns. The second has k rows and j columns. It's going to go through a process which creates a  new matrix that —actually this is not even doing, this is not yet doing the matrix multiplication. This is without the sum. This one's going to create a  new matrix that contains i rows and k, well, how do we say it? i faces and k rows and j columns. So a rank three tensor. So the number of letters is going to be the rank. And the rules of how this works is that if you repeat letters between input arrays,  so here's my inputs, ik and kj, we've got a repeated letter. It means that values along those axes will be multiplied together. So it means that each item in each row of, sorry, in each, yeah, across a row will be  multiplied by each item down each column to create this i by k by j output tensor. So to remind you, our first matrix is 5 by 784. That's m1. Our second matrix is 7084 by 10. That's m2. So i is 5, k is 784 and j is 10. So if I do this torch.einsum,  then I will end up with a i by k by j. It'll be 5 by 784 by 10. And if you have a look, I've run it here on these two tensors, m1 and m2,  and the shape of the result is 5 by 784 by 10. And what it contains is the original 5 rows of m1,  the original 10 columns of m2, and then for the other 784, that dimension,  they're all multiplied together because it's been copied between the two arguments to the einsum. And so if we now sum up that over this dimension, we get back. So what we get back, if we go back to the original matrix multiply we do,  we had 10.94, negative, negative 0.68, et cetera. And so now with this Einstein summation version,  we've got back exactly the same thing. Because what it's done is it's taken each of these  columns by rows, multiplied them together to get this 5 by 784 by 10, and then added up  that 784 for each one, which is exactly what matrix multiplication does. But we're going tu use one of the two things from Einstein summation. The second one says if we omit a letter from the output, so the bit on the right of the arrow,  it means those values will be summed. So if we remove this k, which gives us ik and  kj goes to ij, so we've removed the k entirely, that means that sum happens automatically. So if we run this, as you see, we get back again, matrix multiplication. So Einstein summation notation is, you know, it takes some practice  getting used to, but it's very convenient. And once you get used to it, it's actually a  really nice way of thinking about what's going on. And as we'll see in lots of examples,  often you can really simplify your code by using just a tiny little Einstein summation. And it doesn't even have to be a sum, right? You can, you don't have to omit any letters  if you're just doing products. So maybe it's a bit misnamed. So we can now define our matmul as simply this torch.einsum. So if we now check it, test_close that the original result is equal  to this new matmul. And yes, it is. And let's see how the speed looks. 15 milliseconds. Okay. And that was for the whole thing. So compared to 600 milliseconds. So as you can see, this is much faster than  even the very fast broadcasting approach we used. So this is a pretty good trick, is torch.einsum. Okay. But of course,  we don't have to do any of those things because PyTorch already knows how to do matmul. So there's two ways we can run matmul directly in PyTorch. You can use a special at (@) operator. So x_train @ weights is the same as  matmul(x_train, weights), as you see, test_close. Or you can say torch.matmul. And interestingly, as you can see here, the speed is about the same as the einsum. So there's no particular harm, no particular reason not to do an einsum. So when I say einsum, that stands for Einstein summation notation. All right. Let's go faster still. Currently we're just using my CPU. But I have a GPU. It would be nice to use it. So how does a GPU work? An Nvidia GPU, and indeed, pretty much all GPUs, the way they work is that  they do lots and lots of things in parallel. And you have to actually tell the GPU  what are all the things you want to do in parallel, one at a time. And so what we're going to do is we're going to write in pure Python something that works  like a GPU, except it won't actually be in parallel, so it won't be fast at all. But the first thing we have to do if we're going to get something working in parallel is we have  to create a function that can calculate just one thing, even if a thousand other  things are happening at the same time, it won't interact with anything else. And there's actually a very easy way to think about matrix multiplication in this way,  which is what if we try to create something which just as we've done here fills in a single item  of the result? So how do we create something that just fills in row zero, column zero?  Well what we could do is we could create a new matmul where we're going to pass in the  coordinates of the place that we want to fill in. So we're going to start by passing it (0, 0). We'll pass it the matrix— matrices we want to multiply, and we'll pass in a tensor that we've  pre-filled in with zeros to put the result into. So we're going to say, okay, the result  is torch.zeros, rows by columns, call matmul for location (0, 0),  passing in those two matrices and the bunch of zeros matrix ready to put the result in. And if we call that, we get the answer in cell (0, 0). So here's an implementation of that. So the implementation is first of all,  we've been passed the (0, 0) coordinates. So let's destructure them. So hopefully you've been experimenting with destructuring because it's so important. You see it all the time into i and j. That's the row and the column. Make sure that that is inside the bounds of our output matrix. And we're going to start by start at zero and loop through all of the rows of a  and all of the columns of b for i and j. Sorry, all of the columns of a and all of  the rows of b for i and j, just like the very innermost loop of our very first Python attempt. And then at the end, pop that into the output. So here's something that fills in  one piece of the grid successfully. So we could call this rows by columns times,  each time passing in a different grid. And we could do that in parallel because  none of those different locations interact with any other location. So something which can calculate a little piece of an output on  a GPU is called a kernel. So we'd call this a kernel. And so now we can create something called launch_kernel. We pass it the kernel. So that's the function. So here's an example, launch_kernel passing in the function. And how many rows and how many columns are there in the output grid. And then give me any arguments that you need to calculate it. So in Python, *args just says any additional arguments that you pass  are going to be put into an array called args. If you use something like C, you might've seen  like variadic arguments or parameters. It's the same basic idea. So we're going to be calling launch_kernel. We're going to be saying launch the kernel matmul,  using all the rows of a, all the columns of b, and then the args,  which are going to be the *args, are going to be m1, the first matrix, m2, the second matrix,  and res, another torch.zeros we just created. So launch_kernel is going to loop through  the rows of a, and then for each row of a, it'll loop through the columns of b  and call the kernel, which is matmul, on that grid location passing in m1, m2, and res. So *args here is going to unpack that and pass them as three separate arguments. And if I run that, run all of that,  you'll see it's done it. It's filled in the exact same matrix. Okay, so that's actually not fast at all. It's not doing anything in parallel,  but it's the basic idea. So now to actually do it in  parallel, we have to use something called CUDA. So CUDA is a programming model for Nvidia GPUs. And to program in CUDA from Python, the easiest way currently to do that is with  something called Numba. And Numba is a compiler. Oh, well, you've seen it actually already for non-GPU. It's a compiler that takes Python code and spits out, you know, compiled fast machine code. If you use its CUDA module, it'll actually spit out GPU accelerated CUDA code. So rather than using at @njit like before, we now say @cuda.jit  And it behaves a little bit differently, but you'll see that this matmul,  let me copy the other one over so you can compare, compare it to our Python one. Our Python matmul and this @cuda.jit matmul look, I think identical, except for one thing,  instead of passing in the grid, there's a special magic thing called cuda.grid And you say, how many  dimensions does my grid have? And you unpack it. So that's, you don't have to, it's just a little  convenience that Numba does for you. You don't have to pass over the grid. It passes it over for you. So it doesn't need this grid. Other than that, these two are identical, but the decorator is going to compile that into GPU code. So now we need to create our output tensor just like before,  and we need to do something else, which is we have to take our input matrices and our output. So our input tensors, the matrices in this case, and the output tensor,  and we have to move them to the GPU. Or I should say, copy them to the GPU. So cuda.to_device copies a tensor to the GPU. And so we've got three things  getting copied to the GPU here. And therefore we store the three things over here. Another way I could have written this is I could have said map, which I kind  of quite like doing, a function, which is cuda.to_device to each of these arguments. And this would be the same thing. This is going to call cuda.to_device  on x_train and put it in here on weights and put it in here and on r and put it in here. That's a slightly more convenient way to do it. Okay. So we've got our 50,000 by 10 output. That's just all zeros, of course. That's just how we created it. And now we're going to try and fill it in. There is a particular detail that you don't have  to worry about too much, which is in CUDA. They don't just have a grid,  but there's also a concept of blocks. And there's something we call here TPB, which is threads per block. This is just a detail of the CUDA programming model. You don't have to worry about too much. You can just basically copy this. And what it's going to do is it's going to call each grid item in parallel with a number of  different processes, basically. So this is just the code  which turns the grid into blocks. And so you don't have to worry too  much about the details of that. You just always run it. Okay. And so now how do you call  the equivalent of launch_kernel? Well, it's a slightly weird way to do it, but it works fine. You call matmul, but because matmul has @cuda.jit, it's got a special thing,  which is you have to put something in square brackets afterwards, which is  you have to tell it how many blocks per grid. That's just the result from the previous cell  and how many threads per block in each of the two dimensions. So again, you can just copy and paste this from my version, but then you pass in the  three arguments to the function. This will be a, b,  and c. And this is how you launch a kernel. So this will launch the kernel matmul on the GPU. At the end of it, rg is going to get filled in. It's on the GPU, which is not much good to us. So we now have to copy it back to the CPU, which is called the host, copy to host, to run that. And it's done. And test_close shows us that our  result is similar to our original result. So it seems to be working. So that's great. I see Siva on the  YouTube chat is finding that it's not working on his Mac. That's right. So this will only work on an NVIDIA GPU,  as basically all of the GPU, nearly all the GPU stuff we look at only works on NVIDIA GPUs. Mac GPUs are gradually starting to get a little bit of support from machine learning libraries,  but it's taking quite a while. It's got quite a way to go,  as I say this at least towards the end of 2022. If this works for you later on, that's great. Okay. So let's time how fast that is. Okay. So that was 3.61 milliseconds. And so if we compare that to the PyTorch matmul on CPU, that was 15 milliseconds. So that's great. So it's faster still. So how much faster... Oh, by the way,  we can actually go faster than that, which is we can use the exact same code we had  from the PyTorch up. But here's a trick. If you just take your tensor and write .cuda() after it, it copies it over to the GPU. If it's on a NVIDIA GPU, do the same for weights.cuda() So these are our two CUDA versions. And now I can do the whole thing. And this will actually run on the GPU. And then to copy it back to the host, you just say .cpu() So if we look to see how fast that is,  458 microseconds. So that is... Somebody just pointed out that I wrote the wrong thing here. 1e-3. Okay. So how much faster is that? 458 microseconds, our original,  on the whole dataset, was 663 micromiliseconds. So compared to our broadcast version,  we are another thousand times faster. So overall, this version here  compared to our original version, which was  here... Here, the difference in performance is  5 million X. So when you  see people say, yeah, Python can be pretty slow, it can be better to run stuff on the GPU. If possible, we're not talking about a 20% change. We're talking about a 5 million X change. So that's a big deal. And so that's why you need to be running stuff on the GPU. All right. Some folks on YouTube are wondering how  on earth I'm running CUDA when I'm on a Mac. And given it says local host here. That's because I'm using something called SSH tunneling, which we might get to sometime. I suspect my live coding from the previous course might have covered that already. But this is basically you can use a Jupyter Notebook that's running anywhere in the world  from your own machine using something called SSH tunneling, which is a good thing to look up. Okay. One person asks if Einstein  summation burrows anything from APL. Yes, it does actually. So it's kind of the other way around, actually. APL burrows it from Einstein notation. So I don't know if you remember I mentioned that Ken Iverson, when he developed APL,  was heavily influenced by tensor analysis. And so this Einstein notation is  very heavily used there. If you'll notice, a key thing that  happens in Einstein notation is there's no loop. There isn't this kind of sigma, you know,  i from here to here, and then you put the i inside the function that you're summing up. Everything's implicit. And APL takes that a very long way. And J takes it even further, which is what Ken Iverson developed after APL. And this kind of general idea of removing the index is very important in APL,  and it's become very important in NumPy, PyTorch, TensorFlow, and so forth. So finally, we know how to multiply matrices. Congratulations. So let's practice that. Let's practice what we've learned. So we're going to go to 02_meanshift to practice this. And so we're going to try to exercise our  kind of tensor manipulation operation muscles in this section. And the key actually endpoint for this is the homework. And so what you need to be doing is getting yourself to a point  that you can implement something like this, but for a different algorithm. Why do we care about this? Because this is like learning your times table,  your times tables, if you're doing, you know, mathematics, it's this kind of like  thing that's going to come up all the time. And if you're not good at your times tables,  everything else, a lot, a lot of other things, particularly at primary school and high school,  you know, they get difficult. You get slower and it's frustrating. And you spend time thinking about these mechanical operations rather than getting your work done. It is, it's important that when you have an idea about something you want to try or  debug or profile or whatever, that you can quickly translate that into working code and the way that  code is written for GPUs or even for fast running on CPUs is using broadcasting, Einstein notation,  matrix modifications, and so forth. So you've got to, you've got to,  got to, got to practice super important. So we're going to practice it by  running, by developing a clustering algorithm and the clustering algorithm we're going to work  on is something called mean shift clustering, which hopefully you've never heard of before. And I say that because I just think it's a really fun algorithm,  but not many people have come across, excuse me. And I think you'll find it really useful. So what is cluster analysis? Cluster analysis is very different to anything that we've worked  on in this course so far and that there isn't a dependent variable that we're trying to match. But instead we're just trying to find, are there groups of similar things in this  data and those groups we call clusters. And as you can see from the Wiki page,  there's all kinds of applications of cluster analysis across many different areas. I will say that sometimes cluster analysis can be overused or misused. It's really best for when your,  your various columns are the same kind of thing and have the same kind of scale. For example, pixels are all the same kind of thing. They're all pixels. So one of the examples  they use is market research. So I wouldn't use cluster analysis  for socio-demographic inputs because they're all different kinds of things. But the example they give here makes a lot of sense, which is looking at data from surveys. If you've got a whole bunch of like, from one to five answers on surveys. All right, so let's take a look at this. And the way I like to build my algorithms is  to create some, often, to create some synthetic data that I know how I want it to behave. And so we're going to create six clusters and each class is going to have 750 samples in it. So first of all, I'm going to randomly create six centroids. And so the centroid is going to be like the middle of where my clusters are. So I'm going to randomly create them. I need them n_clusters by 2 because I need an X and a Y coordinate for each one. And so now I'm going to randomly generate data around those six centroids. Okay, so to do that, I'm going to call a little function I made here called sample. And I'm going to run it on each of those six centroids. And so I'll show you what that looks like. So here's what that data looks like. So the Xs are the six centroids  and the colored dots is the data. So if you were given this data without the Xs,  the idea would be to come back with figuring out where the Xs would have been. Like where are these clustering around? And so if you can get clusters, that's the goal here, is to  find out that there's a few discreetly, distinctly different types of data in your data set. So for example, for images, I've used this before to discover that there are some images that look  completely different to all the other ones. For example, they were taken at nighttime  or they're of a different object or something like that. So how does sample work? Well we're passing in the centroid  and so what we want is we're going to get back… So each of those centroids contains an X and a Y. So MultivariateNormal() is just like normal. It's going to give you back normally distributed  data, but more than one item. That's why it's multivariate. And so we passed in two means, a mean for X and a mean for our Y. And so that's the mean that we're going to get. And our standard deviation is going to be five. Why do we use torch.diag( tensor([5., 5.]) )? That's because we're saying,  that's because that for multivariate normal distributions, there's not  just one standard deviation for each column that you get back. There could also be a connection between columns. So columns might not be independent. So you actually need, so it's called a covariance matrix, not just a variance. We discussed that a little bit more in Lesson 9b, if you're interested in learning more about that. Okay, so this is something that's going to give us back random columns of data  with this mean and this standard deviation. And this is the number of samples that we want. And this is coming from PyTorch. So PyTorch has a whole bunch of  different distributions that you can use, which can be very handy. So there's our data. Okay, so remember for clustering,  we don't know the different colors. And we don't know where the Xs are. That's kind of our job is to figure that out. We might just briefly also look at how to plot. So in this case, we want to plot the Xs and we want to plot the data. So it looks like this. So all I do is I loop through each centroid. And I grab that centroid samples, and they're just all done in order. So I grab it from i time n_samples up to (i +1) times n_samples. And then I create a scatterplot with the samples on them. And what I've done is I've created an axis here. And you'll see why later that we can also pass  one in, but I'm not passing one in. So we create a plot and an axis. And so in Matplotlib, you can keep plotting things on the same axis. So then I plot on the centroid, a big X, which is black, and then a smaller X,  which is what is that, magenta. And so that's how I get these Xs. So that's how plot data works. Okay, so how do we create something  now that starts with all the dots and returns where the Xs are?  We're going to use a particular algorithm, particular clustering algorithm called Mean Shift. And  Mean Shift is a nice clustering approach, because you don't have to say how many clusters there are. So it's not that often that you actually got to know how many clusters there are. So we don't have to say. Quite a few things like the  very popular k-means require you to say how many. Instead, we just have to pass them in a bandwidth,  which we'll learn about, which can actually be chosen automatically. And it can also handle clusters of any shape. So they don't have to be ball-shaped  like they are here. They can be kind of like  L-shaped or ellipse-shaped or whatever. And so here's what's going to happen. We're going to pick some point. So let's say we pick that point just there. And so what we now do is we go through each data point. So we'll pick the first one. And so we then find the distance  between that point and every other point. So we're going to have to say,  what is the distance between that point and that point and that point and  that point and that point and that point? And also the ones further away, that point and that point. And you do it for every single point compared to the one that we're currently looking at. Okay, so we get all of those as a big list. And now what we're going to do is we're going to take a weighted average of all of those points. Now that's not interesting without the weighting. If we just take our average of all of the points and how far away they are,  we're going to end up somewhere here, right? This is the average of all the points. But the key is that we're going to take an average. Let me find the right spot. The key is we need to find an average  that is weighted by how far away things are. So for example, this one over here is a very  long way away from our point of interest. And so it should have a very low weight in  the weighted average, whereas this point here, which is very close, should have  a very high weight in our weighted average. So what we do is we create weights for every  point compared to the one that we're currently interested in using what's  called a Gaussian kernel that we'll look at. The key thing to know is that points that are  further away from our point of interest, which is this one, are going to have lower weights. That's what we mean there. They're penalized. The rate at which weights fall to zero is determined by this thing that we set  at the start called the bandwidth. And that's going to be the standard  deviation of our Gaussian. So we take an average of  all the points in the data set, a weighted average weighted by how far away they are. So for our point of interest, this point's going to get a big weight, this point's going to get  a big weight, this point's going to get a big weight, that point's going to get a tiny weight,  that point's going to get an even tinier weight. So it's mainly going to be a weighted average  of these points that are nearby. And the weighted average of those points,  I would guess, is going to be somewhere around about here. And would have a similar thing for the weighted average of the points near this one,  that's going to probably be somewhere around about here, or maybe over here. And so it's going to move all of these points in closer, it's almost like a gravity, right? They're  kind of going to be moved like closer and closer in towards this kind of gravitational center. And then these ones will go towards their own gravitational center, and so forth. Okay, so let's take a look at it. All right, so what's the Gaussian kernel? This is the Gaussian kernel, which was a sign  in the original march for science, back in the days when the idea of not following scientists  was considered socially unacceptable. We used to have much for these things,  if you remember. So this is not normal. So this is the definition of the Gaussian kernel, which is also known as the normal distribution. This is the shape of it. I'm sure you've seen it before. And here is that formula copied directly off the science march sign. Okay, here we are,  you can see the square root, two, pi, et cetera. Okay, and this here is the standard deviation. Now what does that look like? It's very helpful to have something that  we can very quickly plot any function. That doesn't come with Matplotlib,  but it's very easy to write one. Just say, oh, let's, as X, let's use all the  numbers from 0 to 10, 100 of them spaced evenly. That's what linspace() does. Linearly spaced 100 numbers in this range. That's going to be our Xs. So plot those Xs and plot f(X), the Ys. So here's a very nice little plot_func we want. And here it is. And as you can see here,  we've now got something where if you are this, like very close to the point of interest,  you're going to get a very high weight. And if you're a long way away from the  point of interest, you'll get a very low weight. So that's the key thing that we wanted to remember  is something that penalizes further away points more. Now you'll notice here, I managed to plot  this function for a bandwidth of 2.5. And the way I did that was using this  special thing from functools called partial. Now the first thing to point out here is  that very often, drives me crazy, I see people trying to find out what something is in Jupyter. And the way they do it is they'll scroll up to the top of the notebook and search through  the imports and try to find it. That is the dumb way to do it. The smart way to do it is just to type it and press shift enter,  and it'll tell you where it comes from. And you can get its help with question mark  and you can get it source code with two question marks. Okay. So just type it to find out where it comes from. Okay. So this is,  as Siva's mentioned in the chat, also known as currying or partial function application. This creates a new function. So let's just grab it. We create a new function and this function f is, is the function Gaussian, but it's going  to automatically pass bw equals 2.5. This is a partially applied function. So I could type f(4), for example, that's going to be a tensor. There we go. And you can see  that's exactly what this is. Go up to 4, go across. Yep. About 0.44. So we use partial function application all the time. It's a very, very, very important tool. Without it, for example,  plotting this function would have been more complicated. With it, it was trivially easy. I guess the alternative, like one alternative, which would be fine,  but slightly more clunky would be, we could create a little function in line. So we could have said, Oh, plot a function that I'm going to define right now,  which is called lamb—, which is Lambda x, which is Gaussian of  x with a bandwidth of 2.5. You could do that too. You know, it's, it's fine. But, but yeah, partials I think are  a bit neater, a bit less to think about. They often produce some neater and clearer code. Okay. Why did we decide to make the  bandwidth 2.5? As a, as a rule of thumb choose a bandwidth, which covers about a third of the data. So if we kind of found ourselves somewhere over here, right? A bandwidth which covers  about a third of the data would be enough to cover two clusters ish. So you'd want to be kind of like this big. So somewhere in the middle there  so that's the basic idea. Yeah. So, but you can play around with bandwidths and get different amounts of clusters. I should mention, like, often when you see something that's kind of on the complicated side,  like a Gaussian, you can often simplify things. I think most of the implementations and writeups  I've seen talk about using Gaussians, but if you look at the shape of it,  it looks a lot like this shape. So this is a triangular weighting,  which is just using clamp_min(). So it's just using a linear with clamp_min(). And yeah, it occurred to me that we could probably use this just as well. So I decided to define this triangular weighting and then we can try both. Anyway, so we'll start with, we're going to use the Gaussian version. All right. So we're going to be literally moving all the points towards their kind of center of gravity. So we don't want to mess up our original data. So we clone it. That's a PyTorch thing is .clone(). It's very handy. And so big X is our matrix of data. I mean, it's actually a,  that's right, matrix of data. Yeah. And then little x will be our first point. And it's pretty common to use big X,  capital letters for matrices. So this is our data. This is the first point. Okay. So there it is. So we're going to start at (26.2, 26.3). So (26.2, 26.3). So somewhere up here. So little x its shape is just, it's a rank 1 tensor of shape 2,  big X is a rank 2 tensor of 1,500 data points by 2, the x and y. And if we call x[None], that would add a unit access to that. And the reason I'm going to show you that is because we want to find the  distance from little x to everything in big X. And the way we do a distance is with minus,  but you wouldn't be able to go,  you wouldn't be able to go x – X and get the right actually do you get the right answer?  Let's think about that extra shape. Oh, you've got that already. Oh no, actually that is going to work. Isn't it? So  yes. All right. So you can see why we've got these two versions here. If we do x[None], we've got something of shape 1, 2. Now we can subtract that from something of shape 1,500, 2, because the 2s match up  because they're the same and the 1,500 and the 1 matches up because you remember our NumPy rules,  everything matches up to a unit axis. So it's going to copy this  matrix across every row of this matrix and it works. But do you remember there's a special trick, which is if you've got two shapes of different lengths,  we can use the shorter length and it's going to add unit axes to the  front to make it as long as necessary. So we actually don't need the x[None]. We can just use little x and it works because it's going to say, is this compatible with this? Well,  the last axis, remember we go right to left. The last access matches  the second last axis. Oh, it doesn't exist. So we pretend that there's a unit axis and it's going to do exactly the same thing as this. So if you have not studied the broadcasting from last week carefully, that might not  have made a lot of sense to you. And so definitely at this point,  you might want to pause the video and go back and reread the NumPy broadcasting  rules from last time and practice them because that's what we just did. We used NumPy broadcasting rules and we're going to be doing this dozens more times  throughout the rest of the course and many more times in fact in this lesson. Okay. So now I think  it's a pretty good place to have a pause. So I'll see you back here in nine minutes. Hi everybody. Welcome back. So we had got to the point where we had managed to get the distance between our first point x  and all of the other points in the data. And so we're just looking at the  first eight of them here. So the very first distance  is of course zero on the X axis and zero on the Y axis because it is the first point. The other thing is that because we… the way we created the clusters is they're all  kind of next to each other in the list. So these are all in the first cluster. So none of them are too far away from each other. So now that we've got all the distances,  it's easy enough to, well not the distances on X and Y, it's easy enough to get the distance  … kind of Euclidean distance. So we can just square that  difference and sum, and square root. And actually maybe this is a good time  to talk about norms and to talk about what we just did there. We've got all these data points. So here's one of our data points and here's the other one of our data points. And there's some distance across the X axis and there's some distance along the Y axis. We could call that change in X and change in Y. And one way to think about this distance then is it's this distance here. So to calculate that we can use Pythagoras. So a² + b² = c²  Or in our case, so this would be c, a and b, say, so in our case it would be the square root  of the change in X squared plus the change in Y squared. And rather than saying square root, we could say  to the power of a half, another way of saying the same thing. But there's a different way we could find the distance. We could first go along here and then go up here. And so that one would be change in X, if you like, to the one plus change in  Y to the one to the power of one-oneth. I'm writing it a slightly odd way for  reasons you'll see in a moment. It's just this otherwise. In general, if we've got a whole list of numbers, we can add them up. Let's say there are some list V. We can add them up. We can do each one to the power of some number alpha. And take that sum to the one over alpha. And this thing here is called a norm. So you might remember we came across that last week and we come across it again this week. They basically come up, I don't know, they might end up coming up every week. They come up all the time, particularly because the two norm,  which we could write like this (||v||), or we could write like this (|v|),  or we could write like this (|v|2). They're all the two norm. This is just saying it's this equation  for alpha equals two. And Stefano is pointing out we  should actually have an absolute value. I'm not going to worry about that. We're just doing real numbers here. So we'll keep things simple. Oh, well, I guess for higher than one. No, you're probably right. Something like three. Yeah, I guess we do need an absolute value there. That's a good point because, okay, we could have this one. And so the distance actually has to be the absolute value. So the change in X is the absolute value of that distance. Yes, thank you, Stefano. Okay, so we'll have the absolute value. Okay, so the two norm is what happens when alpha equals two. And we would call this, in this case, we would call this the Euclidean distance. But actually where it comes up more often  is when you're doing like a loss function. So the Mean Squared Error  is just, well, the Root Mean Squared Error, I should say, is just the two norm. Whereas the Mean Absolute Error is the one norm. And these are also known as L2 and L1 plus. And remember what we saw in that paper last week, we saw it in this form,  there's a two up here, which is where they got rid of the square root again. So that would have just been  change in X squared plus change in Y squared. And now we don't even need the parentheses. Oopsie-Daisy. Okay. So, all of this is to say that for, you know, this comes up all the time,  because we're very, very often interested in distances and errors and things like that. I'm trying to think, I don't feel like I've ever seen anything other than one or two. So although it is a general concept, I don't think we're going to see  probably things other than one or two in this course. I'd be excited if we do, that would be kind of cool. So here we're taking the Euclidean distance, which is the two norm. So this has got eight things in it, because we've summed it over dimension one. So here's your first homework, is to rewrite using torch.einsum(). You won't be able to get rid of the x minus X, you'll still need to have that in there. But when you've got a multiply followed by a sum, you won't be able to get rid of the  X square root either, you should be able to get rid of the multiply and the sum by  doing it in a single torch.einsum(). So we're summing up over the first  dimension, which is this dimension. So in other words, we're summing up  the X and the Y axis. Okay, so now  we can get the weights by passing those distances into our Gaussian. And so as we would expect, the biggest weights, it gets up to 0.16. So the closest one is itself, it's going to be at a big weight. These other ones get reasonable weights and the ones that are in totally different clusters  have weights small enough that at three significant figures they appear to be zero. Okay, so we've got our weights. So the weights are 1,500 long  vector and of course our original data is 1,500 by 2, the X and the Y for each one. So we now want a weighted average. We want this data, we want  its average weighted by this. So normally an average is the  sum of your data divided by the count. That's a normal average. A weighted average, each item in your data, let's put some i’s around here  just to be more clear, each item in your data is going to have a different weight. And so you multiply each one by the weights. And so rather than dividing by n,  which is just the sum of ones, we would divide by the sum of weights. So this is an important concept to be familiar with, weighted averages. So we need to multiply every one of these x’s by this. Okay, so can we say weight times X? No. All right, why didn't that work?  So, remember we go right to left. So first of all, it's going to say, let's  look at the 2 and multiply that by the 1,500. Are they compatible? These are compatible if  they're equal or if at least one of them is 1. These are not equal and they're not 1,  so they're not compatible. That's why it says the size of a tensor a  must match. Now when it says match,  it doesn't mean they have to be the same. One of them can be 1. Okay, that's what it means to match. They're either equal or one of them is 1. So that doesn't work. On the other hand,  what if this was 1,500 comma 1? If it was 1,500 comma 1,  then they would match because the 1 and the 2 match because one of them  is a unit axis and the 1,500 and the 1,500 match because they're the same. So that's what we're going to do. Because that would then copy this  to every one of these, which is what we want. We want weights for each of these X, Y tuples. So to add the trailing unit axis, we say every row and a trailing unit axis. So that's what that shape looks like. So we can now multiply that by X. And as you can see, it's now weighting each of them. And so each of these x’s and y’s down the bottom, they're all zero. So we can sum that up and then divide by the sum of weights. So let's now write a function that puts all this together. So you can see this really important way of like, to me, the only way that makes sense to  do particularly scientific numerical programming, I actually do all my programming this way,  but particularly scientific and numerical programming is write it all out step by step,  check every piece, have it all there documented for you and for others, and then copy the cells,  merge them together and indent them to indent its control right spare  bracket and put a function header on top. So here's all those things we just did. And now rather than just grabbing the first x, we enumerate through all of them. So that's the distance we had before. That's the weight we had before. There's the product we had before. And then finally sum across the rows,  divide by the sum of the weights. So that's going to calculate  for the ith, it's going to move. So it's actually changing capital X. So it's changing the ith thing in capital X so that it's now the weighted sum. Oh, actually, sorry, the weighted average of all of the other data, weighted by how far it is away. So that's going to do a single step. So the mean shift update is extremely  straightforward, which is clone the data, iterate a few times, and do the update. So if we run it, takes 600 milliseconds. And what I've done is I've plotted the centroids moved by two pixels or two, well, not two pixels,  two units so that you can see them. And so you can see the dots is where our data is. And they're dots now because every single data point is on top of each other on a cluster. And so you can see they are now in the correct spots. So it has successfully clustered our data. So that's great news. And so we could test out our hypothesis. Could we use triangular just as well  as we could have used Gaussian? So control slash comments and uncomments. Yep, we got exactly the same results. So that's good. It's really important to know these keyboard shortcuts, hit H to get a list of them. Some things that are really important don't have keyboard shortcuts. So if you click help, edit keyboard shortcuts, there's a list of all the things Jupyter can do. And you can add keyboard shortcuts to things that don't have them. So for example, I always add keyboard shortcuts to run all cells above and run all cells below. As you can see, I type Q and then A for above and Q and then B for below. All right. Now  that was kind of boring in a way because it did five steps. But we just saw the result. What did it look like one step at a time?  This isn't just fun. It's really important to be  able to see things happening one step at a time because there are so many algorithms we do which  are like updating weights or updating data. So for Stable Diffusion, for example,  you're very likely to want to show your incrementally denoising and so forth. So in my opinion, it's important to know how to do animations. And I found the documentation for this unnecessarily complicated because a lot of  it's about how to make them performant. But most of the time we probably  don't care too much about that. So I want to show you a little trick, a simple  way to create animations without any trouble. So Matplotlib.animation has something  called FuncAnimation. That's what we're going to use. To create an animation, you have to create a function. And the function, you're going to be calling FuncAnimation, passing in the name of that  function and saying how many times to run it. And that's what this frames argument. This says run this function this many times. And then create an animation that basically  contains the result of that with a 500 millisecond interval between each one. So what's this do one going to do? To create one frame of animation, we will call our one_update(). Here it is. one_update(). We're going to call this. That's going to update our Xs. And then we're going to have an axis, which we've created here. So we're going to clear whatever was on the plot before and plot our new data on that axis. And then the only other thing you need to do is that the very first time it calls it,  we want to plot it before running. And d is going to be passed automatically the frame number. So for the zeroth frame, we're going to not do the update. We're just going to plot the data as it is already. I guess another way we could have done that would have been just to say if d, then  do the update, I suppose. That should work too. Maybe it's even simpler. Let's see if I just broke it. Okay. So we're going to clone our data. We're going to create our figure in our subplots. We're going to call FuncAnimation() calling do_one 5 times. And then we're going to display the animation. And so let's see. So HTML takes some HTML and displays it. And to_jshtml() creates some HTML. That's why it's created this HTML  includes JavaScript. And so we'll click run  one, two, three, four, five. There's the five steps. So if I click loop, you'll see them running again and again. Fantastic. So that's how easy it is to create a Matplotlib animation. So hopefully now you can use that to play around with some fun Stable Diffusion animations as well. You don't just have to use to_jshtml(). You can also create movies, for example. You can call to_html5_video() would be another option and  you can save an animation as a movie file. So there's all these different options for that,  but hopefully that's enough to get you started. So  for your homework, I would like you when you create your K-means or whatever, to try to create  your own animation or create an animation of some Stable Diffusion thing that you're playing with. So don't forget this important ax.clear(). Without the ax.clear() it prints it on top of  the last one, which sometimes is what you want, to be fair, but in this case, it's not what I wanted. All right. So kind of slow,  half a second for not that much data. I'm sure it would be nice if it was faster. Well, the good news is we can GPU accelerate it. The bad news is it's not going to GPU accelerate that well because of this loop. This is looping 1,500 times. If we, so looping is not going to run on the GPU. So the best we could do with this would be to move all this to the GPU. Now the problem is that calling something on the GPU 1,500 times from Python is a really bad idea  because there's this kind of huge communication overhead of this kind of flow of control and data  switching back between the CPU and the GPU. It's the kernel launching overhead. It's bad news. So you don't want to have  a really big, fast Python loop that inside it calls CUDA code because GPU code. So we need to make all of this run without the loop, which we could do with broadcasting. So let's roll up our sleeves and try to get the broadcast version of this working. So generally speaking, the way we tend to do things with broadcasting  on a GPU is we create batches or mini batches. So to create batches or mini batches, we know we just call them batches nowadays. We create a batch size. So let's say we're  going to do a batch size of five. So we're going to do five at a time. All right. So how do we do five at a  time? This is only doing one at a time. How do we do five at a time?  As before, let's clone our data. And this time little x for our testing. So we're going to do everything ahead of time, little tests as we always do. This is not now X[0] anymore, but it's X[:bs]. So it's the first five. This is now the first five items. Okay. So little x is now a 5 by 2 matrix. This is our mini batch, the first five items. As before our data itself is 1,500 by 2. All right. So we need a distance calculation, but previously our distance calculation.  Previously our distance calculation only worked if little x was a single number  and it returned just the distances from that to everything in big X, but we need  something that's actually going to be, return a matrix, right? We've got, let's say we've got 5  by 2 in little x and then in big X, we've got something much bigger, not to scale obviously. We've got 1500 by 2. And what is the distance between  these two things? Well, if you think about it, there's going to be a distance between  item one and item one, but there's also going to be a distance between item one and item two,  and there's going to be a distance between, let's use a different color for the next one,  item two and item one. Right? So the output of this is actually going to be a matrix. The distances are actually going to give us a matrix where, I mean, it doesn't matter which  way around we do it. We can decide,  but if we do it this way around, for each of the 5 things in the mini batch, there will be 1,500  distances, the distance between every one. So we're going to need to do broadcasting to do this calculation. So  this is the function that we're going to create, and it's going to create this, as you can see,  5 by 1,500 output, but let's see how we get it. So can we do X – x? No, we can't. Why is that? That's because big  X is 1,500 by 2 and little x is 5 by 2. So it's going to look at, remember our rules,  right to left. Are these compatible? Yes, they are. They're the same. These compatible?  No, they're not. Okay, because they're different. So that's not possible to do. What if, though, we wanted to…  what if we insert in big X an axis at the start here  and in little x, we add an axis in the middle here, then now these are compatible  because you've got, they're the same —because I should use arrows really. These are compatible because one of them is a 1.  And these are compatible because one of them is the 1 as well. So they are all compatible. And what it's going to do is it's going to do this  subtraction between these directly. And it's going to copy this across all 1,500 rows. It'll copy it. This is going to be copied. And then this table across 5 rows, and then this will be copied across these 1,500 rows. Because that's what broadcasting does. I mean, it's not really copying, but it's effectively copying. And so that gives us, we can now subtract them,  and that gives us what we wanted, which is 5 by 1,500. And there's also by 2 because there's both the X and the Y. So that's why this works. That's what this is doing here. It's taking this attraction, it's squaring them and then summing over that last shortest axis,  summing over the X and the Y squareds. And then take square root. I don't know why I said torch dot square root. We could have just put dot square  root at the end, but same, same. In fact, it's worth mentioning that. So most things that you can do on tensors, you can either write torch dot as a function,  or you can write it as a method. Generally speaking, both should be fine. Not everything, but most things work in both spots. Okay, so now we've got this matrix, which is 5 by 1,500. And the nice thing is that our Gaussian kernel doesn't actually  have to be changed to get the weights, believe it or not. And the reason for that is —now how do we get the source code? I could move back up there,  or I can just type gaussian?? and see it. And the nice thing is that this is just,  this is a scalar. So it broadcasts over anything. And then this is also just a scalar. So this is all going to work fine without  any fiddling around. Okay. So now we've got a five by 1,500 weight. So that's the weight for each of the 5 things,  our mini batch, for each of the 1,500 things, each of the most compared to. And then we've got the shape of the data itself, X dot shape, which is the 1,500 points. So now we want to apply each one of these weights to each of these columns. So we need to add a unit axis to the end. So to add a unit axis to the end,  we could say [:, :, None], but dot dot dot ( [...] ) means all of the axes up until  however many you need. So in this case,  the last one comma none ( […, None] ). This is going to add an axis to the end. So this is going to turn, this is going to turn  weight dot shape from 5 comma 1,500 to 5 comma 1500 comma 1. And this is going to add an axis to the start. Remember it's the same as X[None, :, :]  And so let's check our rules. Left, right to left. These are compatible because one of them is 1. These are compatible because they're both the same and these are compatible because one of them is 1. Okay. So it's going to be copying each weight across  to each of the X and Y, which is what we want. We want to, we want to weight both of those  components and it's going to copy each of the 1,500 points. Sorry. Each of the point 5 times,  because we do in fact want to weight every one of the 5 things in our mini batch, a separate set of  weights for each of them. So that sounds perfect. So that's how I think through these calculations. Okay. So we can now do that multiplication, which  is going to give us something of 5 by 1,500 by 2, because we end up with the maximum of our ranks. And then we sum up over those 1,500 points and that's going to give us, now, 5 new data points. Now something that you might notice here is that we've got a product and a sum. And when you see a product and a sum that tells you that maybe we should use einsum. So in this case, we've got our weight. We've got 5 by 1,500. So let's call those i and j as for the 5 and 1,500. We've got the X is 1500 by 2. Now we want to take the product of that and that. So we'll need to use the same name for this row. So we use j again. Okay. And then k  is the number of rows. That's the 2. And then we want to end up with i by k. So torch.einsum() gives exactly the same result. That's great. But you might recognize this. That's exactly the same einsum we had just before when we were doing matrix multiplication. Oh, that is a matrix multiplication. We've just reinvented matrix multiplication  using this rather nifty trick. So we could also just use that. And so, again, this is like what I was just playing around with this morning as I started  to look at this and I was thinking like, oh, can we simplify this? I don't like this kind of like  messing around with axes and summing over dimensions and whatnot. And so it's nice to get things down to einsum or, better still, get them down to matrix multipliers. It's just clearer. It's stuff that we all recognize  because we use them all the time. They all work. Performance would be pretty similar, I suspect. Okay. So now that we've got that, we then need to do our sum. And we've got our 5 points. This is our 5 denominators. So we've got our numerator that we calculated up here for our weighted average. The denominator is just the sum of the weights, remember. And so numerator divided by denominator is our answer. So again, we've gone through every step. We've checked out all the dimensions all along the way. So nothing's going to surprise us. Don't try and write a function like  this just bang from scratch, right? You've got to drive yourself crazy. Instead do it step by step. So here's our mean shift algorithm. Clone the data. Go through five iterations. And now go from naught to n, batch size at a time. So Python has something called slices. So we can create a slice  of X starting at 1 up to i plus batch size, right? Unless you've gone past n, which goes use n. And so then we just copying and pasting each of the lines of code that we had before. Actually, I just copy the cells and merge them. Of course, I don't actually copy and paste because it's slow and boring. And there's my final step to create the new X[s]. And so notice here, s is not a single thing. It's a slice of things. You might not have seen  slice before, but this is just internally what Python's doing when you use colon. And it's very convenient when you need to use the same slice multiple times. Okay. So let's do that using CUDA. I would run it first without CUDA,  but I mean, I've done all the steps before, so it should be fine. So pop it on the GPU and run meanshift(). And let's see how long that takes. It takes 1 millisecond. And previously without GPU,  it took 400 milliseconds. And you know, the other thing we should probably  think about doing is looking at other batch sizes as well, because now we're looping over  batches, right? So if we make the batch size bigger, that for loop is going to do less looping. So what if we make that 16? Will that be any faster? I actually never tried this before. That's interesting. It's actually slower. Huh. There you go. Fascinating. What if it was eight? Amazing. So the big batches don't quite seem to  be working so well for some reason. So I wonder if I've... Hang on. What's going on? Why is it  changing how it should be? My batch size was 5. Why is it slower suddenly?  I think it's just a bit varying. That's probably the answer. So it just varies a lot. Okay. So it doesn't seem like changing the batch size is changing much here. So that's fine. So we'll just leave it where it was. And then check looking at the data. Oh, that looks lovely. Oh, I see. Thank you people on YouTube  pointing out that I'm passing batch size. So I actually need to put it here. Right. So if we used a batch size of 5,  no wonder it was messing up. Oh, look at that. I've totally made it slow now. 157 milliseconds. Ha ha. Okay. 64. 13 milliseconds. All right. Finally, that makes much more sense. 256. 1024. Okay. So the bigger, bigger is better. And I guess we could actually do all 5,000 at once probably. Nice. All right. Thank you YouTube friends  for solving that bizarre mystery. Okay. All right. So that's pretty great. I mean, you know, to see  that we can GPU optimize a mean shift. Actually Googled for this to see if it's been done before. And it's the kind of thing that people like write papers about. So I think it's great that we can do it so easily with PyTorch, which is the kind of thing that  previously had been considered, you know, a very challenging academic problem to solve. So maybe you can do something similar with some of these. Now I haven't told you what these are. So part of the homework is to go read  about them and learn about them. DBSCAN, funnily enough, actually is  an algorithm that I accidentally invented and then discovered a year later had already been invented. That was a long time ago. I was playing around with J, which is the  successor to APL on a very old Windows phone. And I had a long plane flight and I came up  with an algorithm and implemented the whole thing on my phone using J  and then discovered a year later that I just invented DBSCAN. This is actually a really cool algorithm and it's got a lot of similarities to Mean Shift. LSH comes up all the time. So that's great. And in fact, I have a strong feeling, and I've been thinking about this for a while,  that something like LSH could be used to speed this whole thing up a lot. Because if you think about it, and again, maybe this already exists, I don't know. But if you think about it, when we did that distance calculation, the vast majority  of the weights are nearly zero. And so it seems pointless to create  that big, you know, kind of eventually 1,500 by 1,500 matrix, that's slow. It would be much better if we just found the ones that were like pretty close by  and just took their average. And so you want an optimized  nearest neighbors, basically. And so this is an example of  something that can give you a kind of a fast nearest neighbors algorithm. Or, you know, there are things like k-d trees and octrees and stuff like that. So if you want to like, have a bonus bonus,  invent a new mean shift algorithm, which picks only the closest points to avoid quadratic  time. All right. So not very often you get an assignment, which is to invent a new mean shift algorithm,  I guess a super super bonus. Super super bonus. Publish a paper that describes it. All right, you definitely get four points if you do that. We'll give you a number of points equal to the impact factor of the journal you get it  published in. Okay. So what I want to do now  is move on to calculus, which for some of us may not be our favorite topic. That's funny, Stefano wrote the einsum version here already. I didn't notice. Okay. Always ahead of his time, that guy. Let's talk about calculus. If you're not super comfortable with derivatives and what they are and why we care, 3Blue1Brown  has a wonderful series called the essence of calculus, which I strongly recommend watching. It's just a pleasure, actually, to watch as it's everything that  is on 3Blue1Brown, a pleasure to watch. And so we're not going to get into backprop today. Instead, we're just going to have a quick chat about calculus. Where do we start? So the good news is, just like you don't have to know much  linear algebra at all, you basically just need to know about matrix multiplication. You also don't need to know much calculus at all. Just derivatives. So let's think about what derivatives are. So I'm going to borrow actually the same  starting point that 3Blue1Brown uses in one of their videos to consider a car. And we're going to see how far away from home it is at various time points. Okay. So after a minute, let's say after a second,  it's traveled 5 meters. And then after 2 seconds,  it's traveled 10 meters. Okay. And after 3 seconds, you can probably guess, it's traveled 15 meters. So there's this concept here of a, got it the wrong way around,  obviously. So time, distance. Okay. So there's this concept of location. It's like how far have you traveled at a particular point in time? So we  can look at one of these points and find out how far that car has gone. We could also take two points and we can say where did it start at the start of those two points  and where did it finish at the end of those two points. And we can say between those two points, how much time passed  and how far did they travel in 2 seconds, they traveled 10 meters. So we could now also say, all right, well, the slope of something is rise over run. Oopsie-Daisy, 10 meters in 2 seconds. And notice we don't just divide the numbers. We also divide the units. We get 5 meters per second. So, this here is now change the dimensions entirely. We're now not looking at distance, but we're looking at speed or velocity. And it's equal to rise over run. It's equal to the rate of change. And what it says, really, is as time, the X axis, goes up by 1 second,  what happens to the distance in meters? As one second passes, how does  the number of meters change?  And so maybe these aren't points at all. Maybe there's a function. Right? It's a continuum of points. And so you can do that for the function. So the function is a function of time. Distance is a function of time. And so we could say, what's the slope of that function?  And we can get the slope from point A to point B using rise over run. So from t1 to t2, the amount of time that's passed is t2 minus t1. That's how much time has passed. Let's say this is t1, this is t2. And the distance that they've traveled, well, they've moved from  wherever they are at the end to wherever they were at the start. So that's the change in distance divided by the change in time. Change in distance divided by change in time. Okay. Let's say that's Y. So another way, now the thing is, when we talk about calculus, we talk about finding a slope. But we talk about finding a slope of something,  that's often more tricky than this, right? We have slopes of things  that look more like this. And we say,  what's this slope? Oops, I'm terrible at drawing. Let's maybe put it over here because  I'm left-handed. What's this slope?  Now what does it mean to have the idea of a velocity  at an exact moment in time? It doesn't mean anything. At an exact moment in time, you're just like, it's frozen, right? What's happening exactly  now? But what you can do is you can say, well, what's the change in time between a bit before  our point and a bit after our point? And what's the change in distance between a bit before our  point and a bit after our point? And so you can do the same kind of rise over run thing,  right? But you can make that distance between t2 and t1 smaller and smaller and smaller. So let's rewrite this in a slightly different way. Let's call the denominator the distance between t1  plus a little bit, we'll call it d. It's that minus t1. So this is t2, right? It's t1 plus a little bit. So we say, oh, here's t1, let's add a little bit. And notice that when we write it this way, let's actually let's do the rest of it. So now f(t2) becomes F( t1 plus a little bit ). And this is the same. And now notice here that t1 plus d minus t1, we can delete all that  because it just comes out to d. So this is another way of calculating the slope of our function. And as d gets smaller and smaller and smaller, we're kind of getting  a triangle that's tinier and tinier and tinier. And it still makes sense, it's still that some  time has passed and the car has moved, right? But it's just smaller and smaller amounts of time. Now if you did calculus at college or at school, you might've done all this  stuff messing around with limits and epsilon, delta and blah, blah, blah. I've got really good news. It turns out  you can actually just think of this d as a really small number, where d is the difference. And so when we calculate the slope,  we can write it in a slightly different way as the change in Y divided by the change in X. This here is the change in Y and this here is the change in X. And so in other words, this here is a very small number, a very small number. And this here is the result in the function of changing by that very small number. And this way of thinking about calculus is known as the calculus of infinitesimals. And it's how Leibniz originally developed it. And it's been turned into a whole theory nowadays. And the reason I talk about it here is because when we do calculus,  you'll see me doing stuff all the time where I act like dX is a really small number. And when I was at school, I was told I wasn't allowed to do that. I've since learned that it's totally fine to do that. So for example, next lesson, we're going to be looking at the chain rule, which looks like this. So dY/dX equals dY/dU times dU/dX.  And I'm just going to say, oh, these two small numbers can cancel out. And that's why obviously they're the same thing. And that's all going to work out nicely. So anywho, what would be very helpful would be if before the next lesson,  if you're not totally up to date with your remembering all the stuff you did in high school  about calculus, is watch the 3Blue1Brown course. We are not going to be looking, I don't think at all, at integration. So you don't have to worry about that. Also we are not going to, on the whole,  be doing any derivatives by hand. So for example, there are rules such as  dY/dX if Y equals X squared is 2X. These kind of rules you're not really going to have to learn  because PyTorch is going to do them all for you. The one that we care about is going to be  the chain rule, but we're going to learn about that next time. Okay, I hope I don't get beaten to a bloody pulp the next time I walk into  a mathematician's conference. I suspect I might,  but hopefully I get away with this. I think it's safe. We'll see how we go. Thanks everybody very much for joining me and really look forward to seeing you next time where  we're going to do back propagation from scratch. We've already learned to multiply matrices,  so once we've got back propagation as well, we'll be ready to train a neural network. All right, thanks all. Bye.